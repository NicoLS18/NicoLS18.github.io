[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Coming soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas Laub-Sabater",
    "section": "",
    "text": "Hello! My name is Nicolas Laub-Sabater and I am an economics and environmental science double major at Pomona College. I enjoy fishing, competing in different sports such as golf and in general just spending time outside. I am also Puerto Rican which has given me the opportunity to be bilingual and I have continued to cherish this gift and hope to improve it when I go abroad to Spain next semester!"
  },
  {
    "objectID": "tt2.html",
    "href": "tt2.html",
    "title": "GPT Detectors",
    "section": "",
    "text": "This data was collected in an experiment to track how effective AI detectors were at truly capturing AI created work and whether non-native english speakers were at a higher likelihood to be flagged as AI. I chose to focus solely on the effectiveness of the AI detectors as I have always assumed that they are quite good at distinguishing work that is done 100% by a human vs an AI.\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n---- Compiling #TidyTuesday Information for 2023-07-18 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"detectors.csv\"\n\n\n\n\n# A tibble: 7 Ã— 2\n  detector      match_percentage\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Crossplag                 50.1\n2 GPTZero                   48.9\n3 HFOpenAI                  51.4\n4 OriginalityAI             59.0\n5 Quil                      47.8\n6 Sapling                   50  \n7 ZeroGPT                   51.7\n\n\n\n\n\n\n\n\n\nIt is interesting to see that these AI detectors are not very good at all. They are supposed to be quite good, especially at detecting when something is written 100% by AI or human. It urges the question of which side is at fault, has AI writing just advanced so much or is AI detecting simply not a very strong technology at the current moment. I have unfortunately missed the true goal of the data set but my initial interest was focused on the true capabilities of this AI detection and in my additional graphs I would focus more on the native/non-native piece of this data.\nThis information comes from the Tidy Tuesday of July 18th 2023: https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-07-18\nThis dataset, created by Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou, was designed to address their hypothesis that GPT detectors exhibit bias against non-native English writers. The authors aim to investigate the fairness and effectiveness of widely-used GPT detectors in distinguishing between AI-generated and human-written content. With the growing reliance on generative language models, the researchers recognize the potential for misuse and are concerned about the impact on non-native English speakers. Through evaluating GPT detectors using writing samples from both native and non-native English writers, they discovered a pattern of misclassification, where non-native English samples were often incorrectly flagged as AI-generated, while native samples were accurately identified. The study also found that simple prompting strategies could reduce this bias and bypass detectors, further highlighting the unintentional penalization of writers with limited linguistic resources. Their findings emphasize the ethical considerations of using such detectors, particularly in evaluative or educational contexts, and raise awareness about the potential exclusion of non-native English speakers from global conversations. https://arxiv.org/abs/2304.02819"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Bobâ€™s Burgers",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load('2024-11-19') \n\n---- Compiling #TidyTuesday Information for 2024-11-19 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"episode_metrics.csv\"\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')\n\nRows: 272 Columns: 8\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (8): season, episode, dialogue_density, avg_length, sentiment_variance, ...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "BobsBurger.html",
    "href": "BobsBurger.html",
    "title": "Olympics",
    "section": "",
    "text": "---- Compiling #TidyTuesday Information for 2024-08-06 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"olympics.csv\"\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” lubridate 1.9.4     âœ” tibble    3.2.1\nâœ” purrr     1.0.2     âœ” tidyr     1.3.1\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "Olympics.html",
    "href": "Olympics.html",
    "title": "Olympics",
    "section": "",
    "text": "The following data is an overall summary of olympic athletes and their bios compiled from decades of Olympic statistics. It includes categories such as team, sport, height and weight. I have decided to take a look at the physical aspects of these athletes to see if there are any differences depending on where they come from around the globe.\n\n\n# A tibble: 271,116 Ã— 15\n      id name     sex     age height weight team  noc   games  year season city \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijiaâ€¦ M        24    180     80 China CHN   1992â€¦  1992 Summer Barcâ€¦\n 2     2 A Lamusi M        23    170     60 China CHN   2012â€¦  2012 Summer Londâ€¦\n 3     3 Gunnar â€¦ M        24     NA     NA Denmâ€¦ DEN   1920â€¦  1920 Summer Antwâ€¦\n 4     4 Edgar Lâ€¦ M        34     NA     NA Denmâ€¦ DEN   1900â€¦  1900 Summer Paris\n 5     5 Christiâ€¦ F        21    185     82 Nethâ€¦ NED   1988â€¦  1988 Winter Calgâ€¦\n 6     5 Christiâ€¦ F        21    185     82 Nethâ€¦ NED   1988â€¦  1988 Winter Calgâ€¦\n 7     5 Christiâ€¦ F        25    185     82 Nethâ€¦ NED   1992â€¦  1992 Winter Albeâ€¦\n 8     5 Christiâ€¦ F        25    185     82 Nethâ€¦ NED   1992â€¦  1992 Winter Albeâ€¦\n 9     5 Christiâ€¦ F        27    185     82 Nethâ€¦ NED   1994â€¦  1994 Winter Lillâ€¦\n10     5 Christiâ€¦ F        27    185     82 Nethâ€¦ NED   1994â€¦  1994 Winter Lillâ€¦\n# â„¹ 271,106 more rows\n# â„¹ 3 more variables: sport &lt;chr&gt;, event &lt;chr&gt;, medal &lt;chr&gt;\n\n\n\n\n\n\n\n\n\n\n\nAs expected there is a fairly strong relationship between height and weight although certainly some outliers exist. Interestingly these outliers tend to be from the USA although there is a bias as there are overall more USA data points than either of the other countries. Other trends that are noticeable are females tend to be lighter and shorter although there is one outlier male that may be the lightest of all the data points. Additionally, china has a larger proportion of of data points that are taller than the rest which was surprising to me as I expected that to not be the case. And of course the Americans take the trophy for heaviest people, furthering the stereotype that Americans are fat even though obviously these are elite athletes.\nThis comes from the tidy tuesday of August the 6th in 2024: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-08-06/readme.md This data specifically comes from RGriffin, it offers a comprehensive historical record of the modern Olympic Games, covering every event from the first Games in Athens in 1896 up to the Rio Games in 2016. The data was collected by the author through web scraping of www.sports-reference.com in May 2018. The R code used for both scraping and cleaning the data is available on GitHub. Itâ€™s highly recommended to review the authorâ€™s kernel before starting your own analysis, as it provides valuable insights and methodologies that can enhance your work with this dataset."
  },
  {
    "objectID": "Obama.html",
    "href": "Obama.html",
    "title": "Obama",
    "section": "",
    "text": "The data used in this analysis comes from the Obama Presidential Library, specifically from their Digital Research Room. It is publicly available at:\nğŸ”— https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media\nThe dataset is titled tweets.csv, which contains tweets posted from the official @POTUS Twitter account during President Barack Obamaâ€™s administration. The dataset includes columns such as the text of each tweet, timestamp, and other metadata.\nThis analysis focuses primarily on the text column, which contains the actual content of the tweets. The goal is to examine how often President Obama mentioned key policy topics, such as healthcare, climate change, and the economy, over the course of his presidency.\n\n\n\n\n\n\n\n\n\nThis graph illustrates which words Obama used most throughout his presidency in his tweets while not including â€œtheâ€, â€œandâ€, â€œtoâ€, â€œofâ€, â€œinâ€, â€œaâ€, â€œonâ€, â€œforâ€, â€œwithâ€, â€œisâ€, â€œthatâ€,â€œsâ€, â€œatâ€, â€œpotusâ€, â€œrtâ€, â€œampâ€, â€œitâ€,â€œthisâ€, â€œareâ€ and â€œâ†’â€.\n\n\n# A tibble: 9 Ã— 2\n  policy      count\n  &lt;chr&gt;       &lt;int&gt;\n1 jobs          953\n2 economy       661\n3 tax           420\n4 education     399\n5 climate       318\n6 gun           272\n7 immigration   226\n8 poverty       144\n9 healthcare     46\n\n\nThe number of times each of the following words is found exactly like this in unique tweets. Eliminates taxpayers and other additions to the words."
  }
]