[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Coming soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas Laub-Sabater",
    "section": "",
    "text": "Hello! My name is Nicolas Laub-Sabater and I am an economics and environmental science double major at Pomona College. I enjoy fishing, competing in different sports such as golf and in general just spending time outside. I am also Puerto Rican which has given me the opportunity to be bilingual and I have continued to cherish this gift and hope to improve it when I go abroad to Spain next semester!"
  },
  {
    "objectID": "tt2.html",
    "href": "tt2.html",
    "title": "GPT Detectors",
    "section": "",
    "text": "This data was collected in an experiment to track how effective AI detectors were at truly capturing AI created work and whether non-native english speakers were at a higher likelihood to be flagged as AI. I chose to focus solely on the effectiveness of the AI detectors as I have always assumed that they are quite good at distinguishing work that is done 100% by a human vs an AI.\n\n\nCode\ndf&lt;- detectors\nmatch_percentage_by_model &lt;- detectors |&gt;\n  \n  group_by(detector) |&gt;\n  \n#statistically analyzing the data\n  summarise(match_percentage = mean(kind == .pred_class, na.rm = TRUE) * 100)\n\n\nprint(match_percentage_by_model)\n\n\n# A tibble: 7 √ó 2\n  detector      match_percentage\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Crossplag                 50.1\n2 GPTZero                   48.9\n3 HFOpenAI                  51.4\n4 OriginalityAI             59.0\n5 Quil                      47.8\n6 Sapling                   50  \n7 ZeroGPT                   51.7\n\n\nCode\n#graph\nggplot(match_percentage_by_model, aes(x = detector, y = match_percentage, fill = detector)) +\n  geom_bar(stat = \"identity\") +  # Bar plot\n  theme_minimal() +  # Minimal theme\n  labs(\n    title = \"Match Percentage by AI Model\",\n    x = \"AI Model\",\n    y = \"Match Percentage\"\n  )\n\n\n\n\n\n\n\n\n\nIt is interesting to see that these AI detectors are not very good at all. They are supposed to be quite good, especially at detecting when something is written 100% by AI or human. It urges the question of which side is at fault, has AI writing just advanced so much or is AI detecting simply not a very strong technology at the current moment. I have unfortunately missed the true goal of the data set but my initial interest was focused on the true capabilities of this AI detection and in my additional graphs I would focus more on the native/non-native piece of this data.\nThis information comes from the Tidy Tuesday of July 18th 2023: https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-07-18\nThis dataset, created by Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou, was designed to address their hypothesis that GPT detectors exhibit bias against non-native English writers. The authors aim to investigate the fairness and effectiveness of widely-used GPT detectors in distinguishing between AI-generated and human-written content. With the growing reliance on generative language models, the researchers recognize the potential for misuse and are concerned about the impact on non-native English speakers. Through evaluating GPT detectors using writing samples from both native and non-native English writers, they discovered a pattern of misclassification, where non-native English samples were often incorrectly flagged as AI-generated, while native samples were accurately identified. The study also found that simple prompting strategies could reduce this bias and bypass detectors, further highlighting the unintentional penalization of writers with limited linguistic resources. Their findings emphasize the ethical considerations of using such detectors, particularly in evaluative or educational contexts, and raise awareness about the potential exclusion of non-native English speakers from global conversations. https://arxiv.org/abs/2304.02819"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Bob‚Äôs Burgers",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load('2024-11-19') \n\n---- Compiling #TidyTuesday Information for 2024-11-19 ----\n--- There is 1 file available ---\n\n\n‚îÄ‚îÄ Downloading files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  1 of 1: \"episode_metrics.csv\"\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')\n\nRows: 272 Columns: 8\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (8): season, episode, dialogue_density, avg_length, sentiment_variance, ...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "BobsBurger.html",
    "href": "BobsBurger.html",
    "title": "Olympics",
    "section": "",
    "text": "---- Compiling #TidyTuesday Information for 2024-08-06 ----\n--- There is 1 file available ---\n\n\n‚îÄ‚îÄ Downloading files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\n  1 of 1: \"olympics.csv\"\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.4     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "Olympics.html",
    "href": "Olympics.html",
    "title": "Olympics: Height and Weight",
    "section": "",
    "text": "This is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. I scraped this data from¬†www.sports-reference.com¬†in May 2018. It includes categories such as team, sport, height and weight. I have decided to take a look at the physical aspects of these athletes to see if there are any differences depending on where they come from around the globe.\n\n\n# A tibble: 271,116 √ó 15\n      id name     sex     age height weight team  noc   games  year season city \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijia‚Ä¶ M        24    180     80 China CHN   1992‚Ä¶  1992 Summer Barc‚Ä¶\n 2     2 A Lamusi M        23    170     60 China CHN   2012‚Ä¶  2012 Summer Lond‚Ä¶\n 3     3 Gunnar ‚Ä¶ M        24     NA     NA Denm‚Ä¶ DEN   1920‚Ä¶  1920 Summer Antw‚Ä¶\n 4     4 Edgar L‚Ä¶ M        34     NA     NA Denm‚Ä¶ DEN   1900‚Ä¶  1900 Summer Paris\n 5     5 Christi‚Ä¶ F        21    185     82 Neth‚Ä¶ NED   1988‚Ä¶  1988 Winter Calg‚Ä¶\n 6     5 Christi‚Ä¶ F        21    185     82 Neth‚Ä¶ NED   1988‚Ä¶  1988 Winter Calg‚Ä¶\n 7     5 Christi‚Ä¶ F        25    185     82 Neth‚Ä¶ NED   1992‚Ä¶  1992 Winter Albe‚Ä¶\n 8     5 Christi‚Ä¶ F        25    185     82 Neth‚Ä¶ NED   1992‚Ä¶  1992 Winter Albe‚Ä¶\n 9     5 Christi‚Ä¶ F        27    185     82 Neth‚Ä¶ NED   1994‚Ä¶  1994 Winter Lill‚Ä¶\n10     5 Christi‚Ä¶ F        27    185     82 Neth‚Ä¶ NED   1994‚Ä¶  1994 Winter Lill‚Ä¶\n# ‚Ñπ 271,106 more rows\n# ‚Ñπ 3 more variables: sport &lt;chr&gt;, event &lt;chr&gt;, medal &lt;chr&gt;\n\n\n\n\nCode\nsubset_olympics &lt;- olympics |&gt; \n  dplyr::filter(team %in% c(\"United States\", \"China\", \"Netherlands\"))\nggplot(subset_olympics, aes(x=height, y=weight, color=team, shape=sex))+\ngeom_point(alpha=0.7)\n\n\n\n\n\n\n\n\n\nAs expected there is a fairly strong relationship between height and weight although certainly some outliers exist. Interestingly these outliers tend to be from the USA although there is a bias as there are overall more USA data points than either of the other countries. Other trends that are noticeable are females tend to be lighter and shorter although there is one outlier male that may be the lightest of all the data points. Additionally, china has a larger proportion of of data points that are taller than the rest which was surprising to me as I expected that to not be the case. And of course the Americans take the trophy for heaviest people, furthering the stereotype that Americans are fat even though obviously these are elite athletes.\nThis comes from the tidy tuesday of August the 6th in 2024: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-08-06/readme.md This data specifically comes from RGriffin, it offers a comprehensive historical record of the modern Olympic Games, covering every event from the first Games in Athens in 1896 up to the Rio Games in 2016. The data was collected by the author through web scraping of www.sports-reference.com in May 2018. The R code used for both scraping and cleaning the data is available on GitHub. It‚Äôs highly recommended to review the author‚Äôs kernel before starting your own analysis, as it provides valuable insights and methodologies that can enhance your work with this dataset."
  },
  {
    "objectID": "Obama.html",
    "href": "Obama.html",
    "title": "Obama",
    "section": "",
    "text": "Code\n#libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(lubridate)\n\n\nThe data used in this analysis comes from the Obama Presidential Library, specifically from their Digital Research Room. It is publicly available at:\nüîó https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media\nThe dataset is titled tweets.csv, which contains tweets posted from the official @POTUS Twitter account during President Barack Obama‚Äôs administration. The dataset includes columns such as the text of each tweet, timestamp, and other metadata.\nThis analysis focuses primarily on the text column, which contains the actual content of the tweets. The goal is to examine how often President Obama mentioned key policy topics, such as healthcare, climate change, and the economy, over the course of his presidency. The analysis finishes with a study of when Obama tweets, looking for trends and then attempting to put a reason on these trends.\n\n\nCode\n#reading in data\nobamaa &lt;- readr::read_csv('tweets.csv')\nhead(obamaa)\n\n\n# A tibble: 6 √ó 10\n  tweet_id in_reply_to_status_id in_reply_to_user_id timestamp      source text \n     &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;\n1  7.99e17                    NA                  NA 2016-11-16 15‚Ä¶ \"&lt;a h‚Ä¶ \"\\\"N‚Ä¶\n2  7.99e17                    NA                  NA 2016-11-16 15‚Ä¶ \"&lt;a h‚Ä¶ \"\\\"W‚Ä¶\n3  7.99e17                    NA                  NA 2016-11-16 15‚Ä¶ \"&lt;a h‚Ä¶ \"\\\"T‚Ä¶\n4  7.99e17                    NA                  NA 2016-11-16 15‚Ä¶ \"&lt;a h‚Ä¶ \"RT ‚Ä¶\n5  7.99e17                    NA                  NA 2016-11-16 14‚Ä¶ \"&lt;a h‚Ä¶ \"\\\"D‚Ä¶\n6  7.99e17                    NA                  NA 2016-11-16 14‚Ä¶ \"&lt;a h‚Ä¶ \"\\\"W‚Ä¶\n# ‚Ñπ 4 more variables: retweeted_status_id &lt;dbl&gt;,\n#   retweeted_status_user_id &lt;dbl&gt;, retweeted_status_timestamp &lt;chr&gt;,\n#   expanded_urls &lt;chr&gt;\n\n\n\n\nCode\n#cleaning the data for analysis\ncleanobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),\n         text = str_replace_all(text, \"http[[:alnum:][:punct:]]*\", \"\"),  \n         text = str_replace_all(text, \"[[:punct:]]\", \" \"),\n         text = str_replace_all(text, \"‚Äî@POTUS.*\", \"\"),\n         text= str_replace_all(text, \"\\n\", \"\")) \n\n#what moves to remove from the analysis\nstopwords &lt;- c(\"the\", \"and\", \"to\", \"of\", \"in\", \"a\", \"on\", \"for\", \"with\", \"is\", \"that\",\"s\", \"at\", \"potus\", \"  \", \"rt\", \"amp\", \"it\",\"this\", \"are\",\"‚Üí\")\n\n#filtering out stopwords and counting the words\nword_counts &lt;- cleanobama |&gt;\n  mutate(words = str_split(text, \"\\\\s+\")) |&gt;  \n  unnest(words) |&gt;  \n  filter(words != \"\", !words %in% stopwords) |&gt;  \n  count(words, sort = TRUE)  \n\n#creating the graph \nword_counts |&gt;\n  slice_max(n, n = 10) |&gt; \n  ggplot(aes(x = reorder(words, n), y = n)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Words in Obama's Tweets\",\n       x = \"Word\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThis graph illustrates which words Obama used most throughout his presidency in his tweets while not including ‚Äúthe‚Äù, ‚Äúand‚Äù, ‚Äúto‚Äù, ‚Äúof‚Äù, ‚Äúin‚Äù, ‚Äúa‚Äù, ‚Äúon‚Äù, ‚Äúfor‚Äù, ‚Äúwith‚Äù, ‚Äúis‚Äù, ‚Äúthat‚Äù,‚Äús‚Äù, ‚Äúat‚Äù, ‚Äúpotus‚Äù, ‚Äúrt‚Äù, ‚Äúamp‚Äù, ‚Äúit‚Äù,‚Äúthis‚Äù, ‚Äúare‚Äù and ‚Äú‚Üí‚Äù. This graph is not especially helpful as the most used words are not very descriptive of what is going on in the tweets. What we can learn is that many of his tweets are about current news as he utilizes ‚Äútoday‚Äù heavily and they tend to be directed to a specific audience. By utilizing ‚Äúwe‚Äù and ‚Äúour‚Äù it illustrates that Obama is not just sharing opinions but directing messages to the general public. I do not have a clear explanation why Obama and president are utilized so heavily although I assume that Obama tended to share quotes from himself or articles written which therefore would cite him.\n\n\nCode\n#words to focus on \npolicy_keywords &lt;- c(\"healthcare\", \"education\", \"climate\", \"economy\", \n                     \"jobs\", \"tax\", \"immigration\", \"gun\", \"poverty\")\n\n\nobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),  # stringr\n         text = str_replace_all(text, \"[[:punct:]]\", \" \"))  #stringr\n\n# regular expressions\nadd_word_boundaries &lt;- function(word) {\n  paste0(\"(?&lt;!\\\\w)\", word, \"(?!\\\\w)\")  #look behind lookaround\n}\n\n\npolicy_regexes &lt;- setNames(map(policy_keywords, add_word_boundaries), policy_keywords)\n\n\npolicy_counts &lt;- map_dfr(names(policy_regexes), ~{\n  keyword_regex &lt;- policy_regexes[[.x]]\n\n\n  obama |&gt;\n    mutate(has_word = str_detect(text, regex(keyword_regex))) |&gt;\n    summarize(policy = .x,\n              count = sum(has_word))  \n}) |&gt;\n  arrange(desc(count))\n\n\nprint(policy_counts)\n\n\n# A tibble: 9 √ó 2\n  policy      count\n  &lt;chr&gt;       &lt;int&gt;\n1 jobs          953\n2 economy       661\n3 tax           420\n4 education     399\n5 climate       318\n6 gun           272\n7 immigration   226\n8 poverty       144\n9 healthcare     46\n\n\nThe number of times each of the following words is found exactly like this in unique tweets. Eliminates taxpayers and other additions to the words. I decided to create this plot because it highlighted which political themes he addressed most heavily in his tweets and is much more useful than the earlier utilized graph. The top three results are heavily tied together so I am not surprised to see them at the top as I am sure they would all appear together in each unique tweet.\n\n\nCode\nobamatime &lt;- obamaa |&gt;\n  # Split the timestamp into date, time, timezone\n  separate(timestamp, into = c(\"date\", \"time\", \"timezone\"), sep = \"\\\\s+\") |&gt;\n  \n  mutate(time = str_replace_all(time, \"[^0-9:]\", \"\")) |&gt;  \n  \n  # Turn time into an HMS object and create hour and time_chunk columns\n  mutate(tlsime = hms(time),  \n         hour_of_day = hour(tlsime),  \n         time_chunk = cut(hour_of_day, breaks = seq(0, 24, by = 4), \n                          labels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                     \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"), \n                          include.lowest = TRUE)) |&gt; \n  mutate(time_chunk = factor(time_chunk, levels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                                    \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"))) |&gt;  \n  arrange(time_chunk) \n\n# Now count the tweets by time of day\ntweetsbytime &lt;- obamatime |&gt;\n  count(time_chunk)  \n\n#graph\ntweetsbytime |&gt;\n  ggplot(aes(x = time_chunk, y = n)) +\n  geom_col(fill = \"steelblue\") +\n\n  labs(title = \"Obama Tweets by Time of Day (4-hour Chunks)\",\n       x = \"Time of Day (4-hour intervals)\",\n       y = \"Number of Tweets\")\n\n\n\n\n\n\n\n\n\nI then began to work with the timestamp column of the table, first searching for any trends within the time of day in which he would post. I assumed late afternoon as he finished up the structured work day and moved into the later hours of the day in which he may have more freedom to catch up on twitter. This in turn would increase the likelihood that he would see something to repost or share thoughts on issues he had worked with that day. This hypothesis was shown to be correct although I was surprised to see a much larger portion of late night tweets than mid-late morning tweets.\n\n\nCode\n#convert to time format\nobamatime &lt;- obamatime |&gt;\n  mutate(date = as.Date(date)) \n\n\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month_year = format(date, \"%Y-%m\")) |&gt;  \n  count(month_year)  \n\n#graph\ntweetsbymonth |&gt;\n  ggplot(aes(x = month_year, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month Over His Whole Presidency\",\n       x = \"Month-Year\",\n       y = \"Number of Tweets\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nI was then interested if there were specific times of the year which brought upon extra tweeting from Obama but accidentally did not group by month so I had this messy graph. I was going to delete it but I found this trend quite interesting that the longer he was in office the more tweets he was sending out so I left this even though the x-axis is unclear. That jumble of numbers is the year followed by the month. For example 2012-02.\n\n\nCode\n#convert to time format\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month = format(date, \"%m\")) |&gt;\n  count(month)  \n\n#graph\ntweetsbymonth |&gt;\n  ggplot(aes(x = month, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month\",\n       x = \"Month\",\n       y = \"Number of Tweets\") +\n  scale_x_discrete(labels = c(\"01\" = \"January\", \"02\" = \"February\", \"03\" = \"March\", \n                              \"04\" = \"April\", \"05\" = \"May\", \"06\" = \"June\",\n                              \"07\" = \"July\", \"08\" = \"August\", \"09\" = \"September\", \n                              \"10\" = \"October\", \"11\" = \"November\", \"12\" = \"December\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nThis is the graph I was actually hoping to create earlier and found that it is much less helpful in showing any real trends. I found it interesting that January had the most tweets and maybe that was due to issuing plans or goals for the new year or possibly sharing information and reports about the inauguration. But what did surprise was that it looked like a ton of tweets were being sent by him so that led me to the following calculation just out of interest.\n\n\nCode\ntotal_tweets &lt;- nrow(obamatime)\n\ntotal_days &lt;- n_distinct(obamatime$date)\n\naverage_tweets_per_day &lt;- total_tweets / total_days\n\n\naverage_tweets_per_day\n\n\n[1] 10.67786\n\n\nThis function found that Obama averaged 10.68 tweets per day which seems pretty ridiculous to me although I do understand that often he was just reposting something that had been created by others. I also understand this is supposed to be an informal type of sharing information so these may be quick thoughts that he quickly types up and sends out. Lastly, it would not be surprising if there are other individuals who post for him so many of the tweets may not be manually handed by Obama."
  },
  {
    "objectID": "simulationproject.html",
    "href": "simulationproject.html",
    "title": "Will I ever get a single??",
    "section": "",
    "text": "Last semester I was given one of the worst possible times, ending up in the last 15 mins of the nearly 4 hour process. My aspirations of getting a single dashed in an instant when I checked my time. Now as I look ahead for the following years I am still looking to occupy my own room. I understand there are now more options such as a suite or going off campus but in order to keep the simulation effective and repeatable with map I utilized the same concept for all three years. (Freshman year there was no room draw). I estimated that there were around 70 singles opportunities for the 400 students we have in each grade. I then simulated each and then calculated the percentage of students that get a a single at least once in their three years here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n \n  students_picked &lt;- rep(FALSE, total_students)\n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    students_picked[selected_students] &lt;- TRUE\n    \n  \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    \n    num_draws &lt;- num_draws + 1\n  }\n  \n \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\n  map2(simulations, 1:3, ~ {\n    \n    order_of_picking &lt;- .x$order_of_picking\n    draw_number &lt;- 1:length(order_of_picking)\n    \n    \n    colors &lt;- ifelse(order_of_picking &lt;= 70, \"red\", \"blue\")\n    \n   \n    tibble(student = order_of_picking, draw_number = draw_number, color = colors) %&gt;%\n      ggplot(aes(x = draw_number, y = student, color = color)) +\n      geom_point() +\n      labs(title = paste(\"Room Selection Simulation\", .y), x = \"Draw Number (Time Interval)\", y = \"Student ID\") +\n      theme_minimal() +\n      scale_color_identity() + \n      theme(legend.position = \"none\")\n  })\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n \n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n\n  while (sum(students_picked) &lt; total_students) {\n \n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n\n    students_picked[selected_students] &lt;- TRUE\n    \n    \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n  \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nred_dot_students &lt;- map(simulations, ~ .x$order_of_picking[1:70])\n\n\nall_red_dots &lt;- unique(unlist(red_dot_students))\n\n\ntotal_students &lt;- 400\nprob_red_dot_at_least_once &lt;- length(all_red_dots) / total_students\nprob_red_dot_at_least_once\n\n\n[1] 0.4375\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n  \n    students_picked[selected_students] &lt;- TRUE\n    \n \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n   \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nstudent_red_dot_count &lt;- rep(0, 400)\n\n\n#utilized walk instead of map here bc I found that it would eliminate the result and would set up the graph without teh additional numbers. \nwalk(simulations, ~ {\n  red_dot_students &lt;- .x$order_of_picking[1:70]\n  student_red_dot_count[red_dot_students] &lt;&lt;- student_red_dot_count[red_dot_students] + 1\n})\n\n\ndf &lt;- tibble(red_dot_count = student_red_dot_count)\n\n\n\nggplot(df, aes(x = red_dot_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Distribution of How Many Times Students Got a Single\",\n       x = \"Times selected for Single\",\n       y = \"Number of Students\") +\n  scale_x_continuous(breaks = 0:3) +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCreated the simulation and ran it 3 times which can be seen in the first three scatter plots although those are not very helpful as it impossible to identify which student ID is which. So then I calculated the overall percentage of someone receiving at least one single throughout the 3 years and it came out to 43%. Then in order to create a more effective visual representation I created a histogram that shows how many times each student got a single. This showed there is even a chance for a student to get a time early enough for a single all 3 room draws. It also showed that there is a large portion of the student body that will never get a good room draw and honestly I think they should change the process a little so that if you have a terrible time the first room draw you should be guaranteed in the first half, much like the lottery for picking classes. With this new information that I have a 43%* chance of getting a single at least once in my three years of room draws I suspect I will be a part of the 57%. It will also all come down to whether study abroad messes up my chances or improves my chances‚Ä¶ We shall see."
  },
  {
    "objectID": "roomselection.html",
    "href": "roomselection.html",
    "title": "roomselection",
    "section": "",
    "text": "Last semester I was given one of the worst possible times, ending up in the last 15 mins of the nearly 4 hour process. My aspirations of getting a single dashed in an instant when I checked my time. Now as I look ahead for the following years I am still looking to occupy my own room. I understand there are now more options such as a suite or going off campus but in order to keep the simulation effective and repeatable with map I utilized the same concept for all three years. (Freshman year there was no room draw). I estimated that there were around 70 singles opportunities for the 400 students we have in each grade. I then simulated each and then calculated the percentage of students that get a a single at least once in their three years here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n  # Simulate the process until all students have picked a room\n  while (sum(students_picked) &lt; total_students) {\n    # Randomly select 5 students who haven't picked a room yet\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    # Mark those students as having picked a room\n    students_picked[selected_students] &lt;- TRUE\n    \n    # Add selected students to the order of picking\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    # Increment the number of draws (or rounds)\n    num_draws &lt;- num_draws + 1\n  }\n  \n  # Return the order of picking and the number of draws\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\n\n  # Plot for each simulation\n  map2(simulations, 1:3, ~ {\n    # Get the order of picking for this simulation\n    order_of_picking &lt;- .x$order_of_picking\n    draw_number &lt;- 1:length(order_of_picking)\n    \n    # Determine the color for each student (red for the first 70, blue otherwise)\n    colors &lt;- ifelse(order_of_picking &lt;= 70, \"red\", \"blue\")\n    \n    # Create a tibble to plot the points\n    tibble(student = order_of_picking, draw_number = draw_number, color = colors) %&gt;%\n      ggplot(aes(x = draw_number, y = student, color = color)) +\n      geom_point() +\n      labs(title = paste(\"Room Selection Simulation\", .y), x = \"Draw Number (Time Interval)\", y = \"Student ID\") +\n      theme_minimal() +\n      scale_color_identity() + \n      theme(legend.position = \"none\")\n  })\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n\n    students_picked[selected_students] &lt;- TRUE\n    \n\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n\n    num_draws &lt;- num_draws + 1\n  }\n  \n\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nred_dot_students &lt;- map(simulations, ~ .x$order_of_picking[1:70])\n\n\nall_red_dots &lt;- unique(unlist(red_dot_students))\n\n\ntotal_students &lt;- 400\nprob_red_dot_at_least_once &lt;- length(all_red_dots) / total_students\nprob_red_dot_at_least_once\n\n\n[1] 0.4375\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n  \n    students_picked[selected_students] &lt;- TRUE\n    \n \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n   \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nstudent_red_dot_count &lt;- rep(0, 400)\n\n#utilized walk instead of map here bc I found that it would eliminate the result and would set up the graph without teh additional numbers. \nwalk(simulations, ~ {\n  red_dot_students &lt;- .x$order_of_picking[1:70]\n  student_red_dot_count[red_dot_students] &lt;&lt;- student_red_dot_count[red_dot_students] + 1\n})\n\n\ndf &lt;- tibble(red_dot_count = student_red_dot_count)\n\n\n\nggplot(df, aes(x = red_dot_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Distribution of How Many Times Students Got a Single\",\n       x = \"Times selected for Single\",\n       y = \"Number of Students\") +\n  scale_x_continuous(breaks = 0:3) +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCreated the simulation and ran it 3 times which can be seen in the first three scatter plots although those are not very helpful as it impossible to identify which student ID is which. So then I calculated the overall percentage of someone receiving at least one single throughout the 3 years and it came out to 43%. Then in order to create a more effective visual representation I created a histogram that shows how many times each student got a single."
  },
  {
    "objectID": "ethical.html",
    "href": "ethical.html",
    "title": "Ethics within Data Science",
    "section": "",
    "text": "04/16/2025\nTitle: Ethical Reflections on the Amazon AI Hiring Tool\nAmazon‚Äôs now-discontinued AI hiring tool offers a powerful example of how data science can unintentionally perpetuate systemic bias when not designed and tested with ethical rigor. Developed to streamline hiring, the system was trained on ten years‚Äô worth of resumes submitted primarily by men‚Äîan input that reflected the company‚Äôs historically male-dominated workforce. As a result, the model penalized resumes that contained terms like ‚Äúwomen‚Äôs‚Äù or references to all-women‚Äôs colleges, thereby disadvantaging female applicants. While the tool was built with the intent to improve hiring efficiency, the flawed composition of its training data allowed past inequalities to become codified in its algorithmic logic.\nThis case exemplifies the ethical mandate to ‚ÄúRecognize and mitigate bias in ourselves and in the data we use.‚Äù By training the model on biased historical data, Amazon effectively taught the system to mimic prior discriminatory hiring patterns. The algorithm‚Äôs design assumed that prior successful candidates were ideal representations of merit, when in reality, that dataset excluded many qualified individuals due to historical gender bias. Amazon‚Äôs failure to interrogate the representativeness of its training data before deploying the system shows how technical sophistication cannot substitute for ethical foresight. Bias in the data is not a peripheral concern‚Äîit is central to whether machine learning systems serve or harm society.\nAnother relevant ethical principle is to ‚ÄúConsider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society.‚Äù The tool‚Äôs outputs directly influenced the opportunities afforded to real applicants, reinforcing structural inequalities in hiring. The decision to automate a process historically shaped by bias‚Äîwithout first addressing that bias‚Äîresulted in a system that actively marginalized qualified candidates. Although the tool promised increased objectivity, it ultimately reproduced the very gender disparities that hiring automation should help eliminate. This underscores the importance of interrogating how data collection and model design choices translate into real-world consequences.\nA further lesson from this example relates to the principle: ‚ÄúBe open to changing our methods and conclusions in response to new knowledge.‚Äù After internal teams discovered the model‚Äôs gender bias, Amazon chose not to deploy the system further. This decision, though belated, demonstrates an openness to reevaluating a flawed product in light of ethical concerns. In contrast to organizations that double down on problematic technologies, Amazon‚Äôs willingness to discontinue the project illustrates the need for ongoing critical review and the flexibility to pivot when systems fail to meet fairness standards. The ethical use of data science requires continual reassessment‚Äînot just during development, but also after deployment, as societal impacts become visible.\nFinally, the Amazon case also invites reflection on the question: ‚ÄúWho was measured? Are those individuals representative of the people to whom we‚Äôd like to generalize or apply the algorithm?‚Äù The training data consisted largely of resumes from male candidates who had previously been hired by Amazon. These individuals formed the benchmark against which new applicants were evaluated, reinforcing an unrepresentative hiring standard. Women and other underrepresented groups were effectively sidelined by a model designed to replicate historical success, rather than broaden it. This mismatch between the training population and the intended applicant pool highlights the dangers of generalizing from narrow or biased datasets. Without deliberate efforts to ensure that training data reflects the diversity of the modern workforce, algorithmic tools risk becoming vehicles for institutionalized exclusion.\nIn sum, the Amazon AI hiring tool illustrates the necessity of embedding ethical reflection into every stage of the data science lifecycle‚Äîfrom data selection to deployment. Recognizing bias, evaluating societal impact, adapting to new information, and scrutinizing the representativeness of our datasets are not just technical tasks; they are moral imperatives. If we are to build AI systems that genuinely advance equity and opportunity, we must approach data science as both a technical and ethical endeavor.\n\nFurthering the above content, my mom came across another article that I found particularly compelling and relevant to these ethical concerns. As a result, I‚Äôve decided to include an additional discussion addressing some of the earlier questions and data ethics values, this time focusing on a new example: the facial recognition AI developed by Clearview AI. Clearview built its database by scraping billions of images from social media platforms‚Äîwithout the consent of the platforms or the individuals depicted. This unauthorized data collection has led to Clearview being fined tens of millions of dollars by multiple countries for violating privacy laws. Clearly, this data should not be used, as it was obtained without informed consent. Despite this, the U.S. government has not only refrained from penalizing Clearview but has actively adopted its technology in major federal agencies, including Immigration and Customs Enforcement (ICE) and the FBI.\nClearview‚Äôs technology is primarily deployed in law enforcement contexts and disproportionately targets marginalized populations, particularly along the U.S. border. One major ethical concern is the reinforcement of a surveillance feedback loop: over-policing certain communities leads to more recorded violations, which in turn justifies increased surveillance. This cycle perpetuates negative stereotypes‚Äîparticularly of migrants‚Äîand may even serve institutional goals like securing greater funding for agencies like ICE. Perhaps most troubling is that the data powering Clearview‚Äôs technology was collected entirely without consent. The company has refused to disclose where the images came from, although it is widely understood that they were scraped from social media platforms.\nDespite global condemnation and legal action from other countries, the U.S. government‚Äôs continued use of Clearview‚Äôs software raises serious concerns about the future of data privacy. While some states have introduced privacy laws, the absence of comprehensive federal legislation is striking. This gap is partly due to political gridlock, but also reflects what legal scholars have called the ‚Äúincreasing complexity of privacy concerns‚Äù (DLA Piper), which has made it difficult to craft and pass unified legislation. As a result, privacy protections in the U.S. remain fragmented and insufficient, with meaningful regulation often stalled or dismissed. Unless these issues are prioritized, we may not see robust federal privacy protections enacted anytime soon‚Äîan alarming prospect given the scale and scope of modern data collection practices.\n\n\n\n\nDastin, Jeffrey. ‚ÄúAmazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.‚Äù Reuters, 10 Oct.¬†2018, www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\nHao, Karen. ‚ÄúPredictive Policing Algorithms Are Racist. They Need to Be Dismantled.‚Äù MIT Technology Review, 5 Feb.¬†2021, www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol.\n‚ÄúClearview AI‚Äôs Facial Recognition Technology Designed for Surveillance of Marginalized Groups, Report Reveals.‚Äù Business & Human Rights Resource Centre, www.business-humanrights.org/en/latest-news/clearview-ais-facial-recognition-technology-designed-for-surveillance-of-marginalized-groups-report-reveals.\nHart, Robert. ‚ÄúClearview AI: Controversial Facial Recognition Firm Fined $33 Million for Illegal Database.‚Äù Forbes, 3 Sept.¬†2024, www.forbes.com/sites/roberthart/2024/09/03/clearview-ai-controversial-facial-recognition-firm-fined-33-million-for-illegal-database.\nDLA Piper. ‚ÄúUnited States - Data Protection Overview.‚Äù DLA Piper Data Protection Laws of the World, www.dlapiperdataprotection.com/?t=law&c=US.\nIBM. ‚ÄúShedding Light on AI Bias with Real World Examples.‚Äù IBM Think Blog, www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples."
  },
  {
    "objectID": "Final6.html",
    "href": "Final6.html",
    "title": "PP Baseball Off Year or Regression to the Mean?",
    "section": "",
    "text": "The baseball team had a record year last year, reaching the finals in Cleveland, Ohio which is two rounds further than they had ever reached in program history. Yet this year the team spirit is that this season has been let down, beginning as a #5 seed in preseason ratings and have since dropped out of the top 35 nationally. This pattern repeats one seen only 3 years prior when the team had a phenomenal year and made a real push into the playoffs and then the following year was a disappointment as their win percentage dropped over 200 points and they secured 10 fewer wins. So the question then becomes are these ‚Äúpoor‚Äù seasons truly below average seasons? Or are these outstanding years the true outliers and the year following is purely regression towards the mean?\nThis was the question I set out to answer. Instead of scraping past season data I chose to utilize current stats that stated the win percentage of Pomona Pitzer vs their opponents. To do this I went into massey ratings (https://masseyratings.com/cbase2024/ncaa-d3/ratings) and found the expected winning percentage of PP baseball against every team they have played and intend to play this season. A note here is that I chose the win percentage that was calculated off of the 2024 season. I chose this data because I did not want the data to be flawed due to the games that had already occurred this season. I wanted to evaluate what Massey Ratings expected to occur for the 2025 season (when Pomona Pitzer Baseball was at its very best).\nUtilizing the win percentages I found on Massey Ratings and utilizing the Pomona Baseball schedule to extract who they played and how many times, I created the following table. A quick note, the win percentage is the percentage chance that Pomona Pitzer will beat them. Again these numbers have changed since Pomona has done poorly compared to expectations this season. For example, at the end of 2024 PP baseball had a 55% chance of beating La Verne, currently Massey Ratings says La Verne has a 57% chance of beating Pomona, (This is an extreme example as La Verne has had an exceptional season so far).\n\n\nCode\n# Create multi-line string with  data\nraw_text &lt;- \"\nCMS 0.56 3\nLa_Verne 0.55 4\nETBU 0.53 3\nOccidental 0.85 3\nCal_Lu 0.60 3\nCal_Tech 0.95 3\nWhittier 0.84 4\nLewis_and_Clark 0.71 3\nPacific_Lutheran 0.73 2\nChapman 0.60 3\nRedlands 0.74 3\nWilliamette 0.60 1\nMIT 0.80 2\nUW_La_Crosse 0.54 1\nTufts 0.65 3\n\"\n\n# Read the data into a data frame\nschedule &lt;- read.table(text = raw_text, header = FALSE, col.names = c(\"team\", \"win_prob\", \"games\"))\n\n\nprint(schedule)\n\n\n               team win_prob games\n1               CMS     0.56     3\n2          La_Verne     0.55     4\n3              ETBU     0.53     3\n4        Occidental     0.85     3\n5            Cal_Lu     0.60     3\n6          Cal_Tech     0.95     3\n7          Whittier     0.84     4\n8   Lewis_and_Clark     0.71     3\n9  Pacific_Lutheran     0.73     2\n10          Chapman     0.60     3\n11         Redlands     0.74     3\n12      Williamette     0.60     1\n13              MIT     0.80     2\n14     UW_La_Crosse     0.54     1\n15            Tufts     0.65     3\n\n\nWith the previous schedule and probability to data frame I then created the code to simulate the season. This is the original code that I would later turn into a function and map it in order to run it 1,000 times. (So these results are a sample simulated season.)\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Create a data frame with schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74,\n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulate the season\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    dplyr::rowwise() |&gt;\n    dplyr::mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    dplyr::ungroup()\n  \n  # Season totals\n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  list(results = results, total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation\nseason &lt;- simulate_season(schedule)\n\n# View detailed results\nprint(season$results)\n\n\n# A tibble: 14 √ó 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1\n\n\nCode\n# Print summary\ncat(\"\\nSimulated Season Summary:\\n\")\n\n\n\nSimulated Season Summary:\n\n\nCode\ncat(\"Total Wins:\", season$total_wins, \"\\n\")\n\n\nTotal Wins: 21 \n\n\nCode\ncat(\"Total Losses:\", season$total_losses, \"\\n\")\n\n\nTotal Losses: 19 \n\n\nTurning it into a function and running it 1000 times which is followed by some basic stats that we care about in this simulation: average wins, standard deviation, median and then it is interesting to see the min and max win seasons.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n#  original schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Your simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins=median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n# Print results\nprint(summary_stats)\n\n\n# A tibble: 1 √ó 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     27.7    2.78          28       36       19\n\n\nThe following graph is a presentation of all 1000 simulations and I made it shiny so that you can interact with it and understand percentage wise how a 27 win year such as this one is an expected year and not a poor showing. These calculations are also inflated as the win percentages were gathered off of the end of a historic PP season.\n\n\nCode\n# Prepare simulation data once\nset.seed(42)\n\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  tibble(total_wins = total_wins)\n}\n\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Simulated Win Distribution\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"win_thresh\", \"Minimum Wins:\",\n                  min = min(sim_results$total_wins),\n                  max = max(sim_results$total_wins),\n                  value = 27,\n                  step = 1)\n    ),\n    mainPanel(\n      textOutput(\"percent_text\"),\n      plotOutput(\"win_plot\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  output$percent_text &lt;- renderText({\n    pct &lt;- mean(sim_results$total_wins &gt;= input$win_thresh) * 100\n    paste0(\"Percentage of seasons with a better record than \", input$win_thresh, \" wins: \", round(pct, 2), \"%\")\n  })\n  \n  output$win_plot &lt;- renderPlot({\n    ggplot(sim_results, aes(x = total_wins)) +\n      geom_bar(fill = \"steelblue\", color = \"black\") +\n      geom_vline(xintercept = input$win_thresh, color = \"red\", linetype = \"dashed\", size = 1) +\n      labs(\n        title = \"Distribution of Total Wins Over 1000 Simulated Seasons\",\n        x = \"Total Wins\",\n        y = \"Frequency\"\n      ) +\n      theme_minimal()\n  })\n}\n\n# Run app\nshinyApp(ui = ui, server = server)\n\n\nhttps://nicols18.shinyapps.io/simulation/ The above code relates to my shiny application.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Schedule data\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  tibble(total_wins = total_wins)\n}\n\n# Run 1000 simulations\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Create a bar plot of total wins\nggplot(sim_results, aes(x = total_wins)) +\n  geom_bar(fill = \"steelblue\", color = \"black\") +\n    geom_vline(xintercept = 27, color = \"red\", linetype = \"dashed\", size = 1)+ \n  labs(\n    title = \"Distribution of Total Wins Over 1000 Simulated Seasons\",\n    x = \"Total Wins\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\npct_27_or_more &lt;- mean(sim_results$total_wins &gt;= 27) * 100\nprint(paste0(\"Percentage of simulations with 27 or more wins: \", round(pct_27_or_more, 2), \"%\"))\n\n\n[1] \"Percentage of simulations with 27 or more wins: 65.7%\"\n\n\nThe red line represents the number of wins they had this season.\nAs can be seen in this graph the final result of 27 wins in the 2025 season is well aligned with the center of the bell curve. It could be considered a little low as the average expected number of wins was 27.7 and the median was 28 but still that is barely under performing. With this data I would suggest that this has not been a poor season but instead just slightly below what their expected number of wins should have been. Especially considering that these predictions were based off the greatest season in Pomona History so it could be safely assumed that these predictions are honestly too high. Last season the team had 30 regular season wins and yet after the season they were still predicted to only have won 28 games so that implies that they outperformed despite putting up exceptional offensive and defensive statistics.  So I decided to get some summary statistics utilizing the new predictive winning percentage after the 2025 season (seen below) and I found that even this year they outperformed what they were predicted to achieve. Albeit they overachieved by less than the 2024 team but it still shows how the 2025 team has had a solid year and to ever have considered it an ‚Äúoff year‚Äù is ridiculous. As I look to the future I wish them the best in the SCIAC playoffs as currently they are not guaranteed a regional spot with the new NPI rating system implemented by the NCAA.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Updated schedule data\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.49, 0.43, 0.51, 0.84, 0.58, 0.93, 0.77,\n               0.64, 0.68, 0.72, 0.64, \n               0.75, 0.48, 0.71),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary statistics\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins = median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n# Print the summary\nprint(summary_stats)\n\n\n# A tibble: 1 √ó 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     26.3    2.79          26       34       17"
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "Different Policing Across the Bay Bridge?",
    "section": "",
    "text": "As an Oakland native I have a very different understanding of the police than some of my peers. My understanding of the police in Oakland is that they are considerably overloaded so calling them for a crime that is not a current threat to someone‚Äôs life is often postponed severely. For example, I have actively seen someone underneath a neighbors car cutting out their catalytic converter and there was no point in calling the police as they would have no interest in such a crime. Even when our car was stolen they took hours to file a report and found it four days later after it had been trashed and discarded. For these reasons I was particularly interested in the percentage of stops that turned into searches and arrests as my understanding would be that a majority of the time they are making stop because they believe they will be taking some amount of action.\nSo my first action was to calculate the percentage of the time that stops turned into a search and the percentage of the time that a stop turned into arrest. This singular number for each was not especially interesting so I decided to see if there were any trends over time and created a scatterplot. This graph was created in intervals of four months because we only have data from 2013-2017. Towards the end we see a dramatic climb and I am very interested to know whether this climb continued. Note that the search rate gets up to nearly 40% in some periods and averages at 30% of the time.\n\n\nCode\nSELECT\n  CONCAT(YEAR(date), '-', LPAD(FLOOR((MONTH(date)-1)/4)*4 + 1, 2, '0')) AS period,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\nGROUP BY period\nORDER BY period\n\n\n\n\nCode\nggplot(oakland_rates, aes(x = period)) +\n  geom_point(aes(y = pct_search, color = \"Search Rate\"), size = 3) +\n  geom_line(aes(y = pct_search, color = \"Search Rate\"), group = 1) +\n  geom_point(aes(y = pct_arrest, color = \"Arrest Rate\"), size = 3) +\n  geom_line(aes(y = pct_arrest, color = \"Arrest Rate\"), group = 1) +\n  labs(\n    title = \"Search and Arrest Rates in Oakland Traffic Stops (2013‚Äì2017)\",\n    x = \"Period (4-month intervals)\",\n    y = \"Percentage (%)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nI also wanted to include a secondary graph to illustrate that the number of stops overall is not correlated to these search and arrest rates which I found interesting. Yet with the normalization of the number of stops I felt it did not illustrate the percentages well which is why I have left both graphs.\n\n\nCode\noakland_rates &lt;- oakland_rates |&gt;\n  mutate(norm_total_stops = 100 * total_stops / max(total_stops))\n\nggplot(oakland_rates, aes(x = period)) +\n  geom_point(aes(y = pct_search, color = \"Search Rate\"), size = 3) +\n  geom_line(aes(y = pct_search, color = \"Search Rate\"), group = 1) +\n  geom_point(aes(y = pct_arrest, color = \"Arrest Rate\"), size = 3) +\n  geom_line(aes(y = pct_arrest, color = \"Arrest Rate\"), group = 1) +\n  geom_point(aes(y = norm_total_stops, color = \"Total Stops (Normalized)\"), size = 3) +\n  geom_line(aes(y = norm_total_stops, color = \"Total Stops (Normalized)\"), group = 1) +\n  labs(\n    title = \"Search, Arrest, and Total Stops in Oakland Traffic Stops (2013‚Äì2017)\",\n    x = \"Period (4-month intervals)\",\n    y = \"Percentage or Normalized Total\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nYet this information did not quite answer my question as a citation would also be warrant for action. So with some help I created a dataframe in which it counted whether any of the following had occurred on the traffic stop: citation, search, or arrest. As you can see in the table the percentage of stops with action was 65% which was honestly surprisingly low to me. There may be a different definition of what is considered a traffic stop and which stops are not included in this data but overall expected something higher.\n\n\nCode\nSELECT\n  COUNT(*) AS total_stops,\n  SUM(\n    CASE\n      WHEN arrest_made = TRUE OR search_conducted = TRUE OR citation_issued = TRUE THEN 1\n      ELSE 0\n    END\n  ) AS stops_with_action,\n  ROUND(100.0 * \n    SUM(\n      CASE\n        WHEN arrest_made = TRUE OR search_conducted = TRUE OR citation_issued = TRUE THEN 1\n        ELSE 0\n      END\n  ) / COUNT(*), 2) AS pct_with_action\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND arrest_made IS NOT NULL\n  AND search_conducted IS NOT NULL\n  AND citation_issued IS NOT NULL;\n\n\n\n\nCode\nprint(`citation/search`)\n\n\n  total_stops stops_with_action pct_with_action\n1      133405             87650            65.7\n\n\nThen I realized this data of search and arrest percentages are baseless without some comparisons. So I chose to compare Oakland to two other Bay Area cities. This would generally help keep constant laws, general size of the cities and overall sentiment of the police force. I compared Oakland to San Francisco (a 10 minute drive across the Bay Bridge) and San Jose (a 45 minute drive south).\n\n\nCode\nSELECT\n  'Oakland' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\nUNION ALL\n\nSELECT\n  'San Francisco' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_francisco_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\nUNION ALL\n\nSELECT\n  'San Jose' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_jose_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\n\n\n\nCode\nprint(city_rates)\n\n\n           city total_stops searches arrests pct_search pct_arrest\n1       Oakland      102690    30275   12353      29.48      12.03\n2 San Francisco      292469    13886    2734       4.75       0.93\n3      San Jose      111706    32998    9999      29.54       8.95\n\n\nThe sharp contrast between San Francisco and Oakland/San Jose was unbelievable to me. First thing to notice is San Francisco had nearly 3 times the number of stops as the others and this was after I accounted for the time periods and only included the overlapping years between all three. Yet despite having three times more total stops they had barely a third of the total searches and obviously a minimal search and arrest percentage. I was also surprised to see how similar San Jose‚Äôs numbers were to Oakland. I do not know San Jose well but my general understanding was that they had some ‚Äúbad‚Äù areas but not enough to keep up with Oakland‚Äôs numbers. I am glad I made this comparison as I found it super interesting and it has left me with more questions to pursue about where these enormous differences stem from. Is it the system in each police force? Different laws? Less crime?\nLastly, it felt essential to take a look at race within the previous stats that I had already been looking at. Unfortunately, as it often shows, Black drivers and Hispanic drivers made up the largest proportion of searched drivers compared to white, asian and other in all three cities. So even in such diverse cities these biases and racism continues to persist in clear ways.\n\n\nCode\nSELECT\n  'Oakland' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\nUNION ALL\n\nSELECT\n  'San Francisco' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_francisco_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\n\nUNION ALL\n\nSELECT\n  'San Jose' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_jose_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\n\n\n\nCode\nggplot(city_race_rates, aes(x = city, y = pct_search, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Search Rate by Race (2013‚Äì2017)\",\n    x = \"City\",\n    y = \"Search Rate (%)\",\n    fill = \"Race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(city_race_rates, aes(x = city, y = pct_arrest, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Arrest Rate by Race (2013‚Äì2017)\",\n    x = \"City\",\n    y = \"Arrest Rate (%)\",\n    fill = \"Race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhat was more unbelievable was the stats on the percentage of stops by race for each city. This was truly eye opening on how biased the law enforcement system can be. I included the demographics of all three cities below the the following bar graph to truly illustrate how far from proportional these stops are. An insane stat I found is that currently there are 90,000 African Americans in Oakland in 2023 (in 2015 I believe there was less which makes this crazier), in the three years in which we are analyzing data the Oakland Police stopped nearly 78,000 African Americans. Of course there were repeats offenders but still the fact that it is possible that over 85% of African Americans in the city were stopped is absurd.\n\n\nCode\ncity_race_pct &lt;- city_race_rates |&gt;\n  group_by(city) |&gt;\n  mutate(pct_total_stops = 100 * total_stops / sum(total_stops))\n\n# Create the grouped bar chart\nggplot(city_race_pct, aes(x = city, y = pct_total_stops, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Percentage of Stops by Race per City (2013‚Äì2017)\",\n    x = \"city\",\n    y = \"Percent of Total Stops\",\n    fill = \"race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOakland Demographics: 30.51% White, 21.09% Black, 19.61% other race, 17.21% Hispanic, 15.54% Asian  San Francisco Demographics: 39.2% white, 34.4% Asian, 15.4% Hispanic, 8.4% other race, 5.2% Black  San Jose Demographics: 33.2% Hispanic, 32.8% Asian, 27.6% White, 2.8% Black, 3.6 % other\nThis demographics data was acquired from Data USA: https://datausa.io/profile/geo/oakland-ca/\nThis was truly an enlightening experience for me as I assumed there was still issues within Oakland‚Äôs system but I assumed they would be lesser because of the enormous diversity. Unfortunately that is not the case.\n\n\nCode\ndbDisconnect(con_traffic)\n\n\nData base from:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al.¬†2020. ‚ÄúA Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.‚Äù Nature Human Behaviour, 1‚Äì10."
  },
  {
    "objectID": "Presentation.html#section",
    "href": "Presentation.html#section",
    "title": "Changes to my Website",
    "section": "",
    "text": "Overall Changes I Made to my Website \nSubstantially improved my citations Added notes within my code to make it clearer *"
  },
  {
    "objectID": "Presentation.html#section-1",
    "href": "Presentation.html#section-1",
    "title": "Changes to my Website",
    "section": "",
    "text": "Creating a new Simulation Initially created a simulation about my odds to receive a single."
  },
  {
    "objectID": "Presentation.html#overall-changes-i-made-to-my-website",
    "href": "Presentation.html#overall-changes-i-made-to-my-website",
    "title": "Changes to my Website",
    "section": "Overall Changes I Made to my Website",
    "text": "Overall Changes I Made to my Website\n\nSubstantially improved my citations\n\nAdded notes within my code to make it clearer\nAdded code folding to all projects and overall cleaned up the presentation\nEdited introductions so they had a professional tone and appearance\nAdjusted home page to describe my current skills and career interests\nAdded Resume"
  },
  {
    "objectID": "Presentation.html#creating-a-new-simulation",
    "href": "Presentation.html#creating-a-new-simulation",
    "title": "Changes to my Website",
    "section": "Creating a new Simulation",
    "text": "Creating a new Simulation\n\nInitially created a simulation about my odds to receive a single.\n\nInsufficient information and an overload of variables hindered simulation value\nAdditionally overall lack of clarity throughout project\n\nInstead I chose to get a better understanding of the Pomona Baseball teams success\n\n\n\n\nPomona Pitzer Baseball sweeping ETBU to head to their first ever College World Series Appearance Photo/courtesy of Pomona-Pitzer athletics"
  },
  {
    "objectID": "Presentation.html#leading-question",
    "href": "Presentation.html#leading-question",
    "title": "Changes to my Website",
    "section": "Leading Question",
    "text": "Leading Question\n\nIs Pomona truly underperforming the year after a historic run?\n\n2022 historic run to regionals\n2023 poor season and missed SCIAC playoffs\n2024 greatest run in history to NCAA World Series in Cleveland\n2025 sentiments of a let down season (although ended up finishing strong)\n\n\n\n\n\nPomona Pitzer final game in Cleveland as they were eliminated by Endicott Photo/courtesy of Pomona-Pitzer athletics"
  },
  {
    "objectID": "Presentation.html#collecting-the-data",
    "href": "Presentation.html#collecting-the-data",
    "title": "Changes to my Website",
    "section": "Collecting the Data",
    "text": "Collecting the Data\n\nUtilized Massey Ratings to collect winning percentages https://masseyratings.com/cbase2024/ncaa-d3/ratings\n\n\n\n# A tibble: 14 √ó 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1"
  },
  {
    "objectID": "Presentation.html#collecting-the-data-and-simulating-the-season",
    "href": "Presentation.html#collecting-the-data-and-simulating-the-season",
    "title": "Changes to my Website",
    "section": "Collecting the Data and Simulating the Season",
    "text": "Collecting the Data and Simulating the Season\n\nUtilized Massey Ratings to collect winning percentages https://masseyratings.com/cbase2024/ncaa-d3/ratings\n\n\n\n# A tibble: 14 √ó 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1"
  },
  {
    "objectID": "Presentation.html#results-of-1000-simulations",
    "href": "Presentation.html#results-of-1000-simulations",
    "title": "Changes to my Website",
    "section": "Results of 1,000 Simulations",
    "text": "Results of 1,000 Simulations\n\nAfter mapping the simulation 1,000 times I found the following basic stats:\n\n\nset.seed(42)  # for reproducibility\n\n#  original schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins=median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n\nprint(summary_stats)\n\n# A tibble: 1 √ó 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     27.7    2.78          28       36       19"
  },
  {
    "objectID": "Presentation.html#graphical-presentation",
    "href": "Presentation.html#graphical-presentation",
    "title": "Changes to my Website",
    "section": "Graphical Presentation",
    "text": "Graphical Presentation\n\nCreated a shiny bar graph\n\nhttps://nicols18.shinyapps.io/simulation/"
  },
  {
    "objectID": "Presentation.html#conclusions",
    "href": "Presentation.html#conclusions",
    "title": "Changes to my Website",
    "section": "Conclusions",
    "text": "Conclusions\n\nFinal result of 27 wins aligns closely with the center of the simulated distribution.\nSlightly underperformed vs.¬†expected average (27.7) and median (28), but the difference is minimal.\nPerformance is respectable, not disappointing‚Äîespecially when considering predictions were based on the historic 2024 season.\nIn 2024, the team won 30 games, yet was predicted for 28‚Äîoverperformed even with elite stats."
  },
  {
    "objectID": "Presentation.html#how-massey-ratings-looks",
    "href": "Presentation.html#how-massey-ratings-looks",
    "title": "Changes to my Website",
    "section": "How Massey Ratings Looks",
    "text": "How Massey Ratings Looks\nhttps://masseyratings.com/game.php?s0=614639&oid0=6286&h=0&s1=614639&oid1=0 &lt;/p&gt;"
  },
  {
    "objectID": "Presentation.html#will-pomona-get-a-regional-bid",
    "href": "Presentation.html#will-pomona-get-a-regional-bid",
    "title": "Changes to my Website",
    "section": "Will Pomona get a Regional Bid?",
    "text": "Will Pomona get a Regional Bid?\n\nGuaranteed bid if they win SCIAC Playoffs starting today at 3pm\nTop 23 teams in terms of NPI that do not win their conference get an at large bid\n64 total teams make the playoffs\nNPI has a final calculation on May 11th\nPomona is currently 54th nationally in NPI. Will they get a bid?"
  },
  {
    "objectID": "Presentation.html#massey-ratings",
    "href": "Presentation.html#massey-ratings",
    "title": "Changes to my Website",
    "section": "Massey Ratings ",
    "text": "Massey Ratings \nhttps://masseyratings.com/game.php?s0=614639&oid0=6286&h=0&s1=614639&oid1=0 &lt;/p&gt;"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "cat('&lt;iframe src=\"Resumewebsite.pdf\" width=\"100%\" height=\"800px\" style=\"border:none;\"&gt;&lt;/iframe&gt;')"
  }
]