[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Coming soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas Laub-Sabater",
    "section": "",
    "text": "Hello! My name is Nicolas Laub-Sabater and I am an economics and environmental science double major at Pomona College. I enjoy fishing, competing in different sports such as golf and in general just spending time outside. I am also Puerto Rican which has given me the opportunity to be bilingual and I have continued to cherish this gift and hope to improve it when I go abroad to Spain next semester!"
  },
  {
    "objectID": "tt2.html",
    "href": "tt2.html",
    "title": "GPT Detectors",
    "section": "",
    "text": "This data was collected in an experiment to track how effective AI detectors were at truly capturing AI created work and whether non-native english speakers were at a higher likelihood to be flagged as AI. I chose to focus solely on the effectiveness of the AI detectors as I have always assumed that they are quite good at distinguishing work that is done 100% by a human vs an AI.\n\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n---- Compiling #TidyTuesday Information for 2023-07-18 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"detectors.csv\"\n\n\n\n\n# A tibble: 7 Ã— 2\n  detector      match_percentage\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Crossplag                 50.1\n2 GPTZero                   48.9\n3 HFOpenAI                  51.4\n4 OriginalityAI             59.0\n5 Quil                      47.8\n6 Sapling                   50  \n7 ZeroGPT                   51.7\n\n\n\n\n\n\n\n\n\nIt is interesting to see that these AI detectors are not very good at all. They are supposed to be quite good, especially at detecting when something is written 100% by AI or human. It urges the question of which side is at fault, has AI writing just advanced so much or is AI detecting simply not a very strong technology at the current moment. I have unfortunately missed the true goal of the data set but my initial interest was focused on the true capabilities of this AI detection and in my additional graphs I would focus more on the native/non-native piece of this data.\nThis information comes from the Tidy Tuesday of July 18th 2023: https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-07-18\nThis dataset, created by Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou, was designed to address their hypothesis that GPT detectors exhibit bias against non-native English writers. The authors aim to investigate the fairness and effectiveness of widely-used GPT detectors in distinguishing between AI-generated and human-written content. With the growing reliance on generative language models, the researchers recognize the potential for misuse and are concerned about the impact on non-native English speakers. Through evaluating GPT detectors using writing samples from both native and non-native English writers, they discovered a pattern of misclassification, where non-native English samples were often incorrectly flagged as AI-generated, while native samples were accurately identified. The study also found that simple prompting strategies could reduce this bias and bypass detectors, further highlighting the unintentional penalization of writers with limited linguistic resources. Their findings emphasize the ethical considerations of using such detectors, particularly in evaluative or educational contexts, and raise awareness about the potential exclusion of non-native English speakers from global conversations. https://arxiv.org/abs/2304.02819"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Bobâ€™s Burgers",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load('2024-11-19') \n\n---- Compiling #TidyTuesday Information for 2024-11-19 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"episode_metrics.csv\"\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')\n\nRows: 272 Columns: 8\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\ndbl (8): season, episode, dialogue_density, avg_length, sentiment_variance, ...\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "BobsBurger.html",
    "href": "BobsBurger.html",
    "title": "Olympics",
    "section": "",
    "text": "---- Compiling #TidyTuesday Information for 2024-08-06 ----\n--- There is 1 file available ---\n\n\nâ”€â”€ Downloading files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n  1 of 1: \"olympics.csv\"\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” lubridate 1.9.4     âœ” tibble    3.2.1\nâœ” purrr     1.0.2     âœ” tidyr     1.3.1\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "Olympics.html",
    "href": "Olympics.html",
    "title": "Olympics",
    "section": "",
    "text": "The following data is an overall summary of olympic athletes and their bios compiled from decades of Olympic statistics. It includes categories such as team, sport, height and weight. I have decided to take a look at the physical aspects of these athletes to see if there are any differences depending on where they come from around the globe.\n\n\n# A tibble: 271,116 Ã— 15\n      id name     sex     age height weight team  noc   games  year season city \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijiaâ€¦ M        24    180     80 China CHN   1992â€¦  1992 Summer Barcâ€¦\n 2     2 A Lamusi M        23    170     60 China CHN   2012â€¦  2012 Summer Londâ€¦\n 3     3 Gunnar â€¦ M        24     NA     NA Denmâ€¦ DEN   1920â€¦  1920 Summer Antwâ€¦\n 4     4 Edgar Lâ€¦ M        34     NA     NA Denmâ€¦ DEN   1900â€¦  1900 Summer Paris\n 5     5 Christiâ€¦ F        21    185     82 Nethâ€¦ NED   1988â€¦  1988 Winter Calgâ€¦\n 6     5 Christiâ€¦ F        21    185     82 Nethâ€¦ NED   1988â€¦  1988 Winter Calgâ€¦\n 7     5 Christiâ€¦ F        25    185     82 Nethâ€¦ NED   1992â€¦  1992 Winter Albeâ€¦\n 8     5 Christiâ€¦ F        25    185     82 Nethâ€¦ NED   1992â€¦  1992 Winter Albeâ€¦\n 9     5 Christiâ€¦ F        27    185     82 Nethâ€¦ NED   1994â€¦  1994 Winter Lillâ€¦\n10     5 Christiâ€¦ F        27    185     82 Nethâ€¦ NED   1994â€¦  1994 Winter Lillâ€¦\n# â„¹ 271,106 more rows\n# â„¹ 3 more variables: sport &lt;chr&gt;, event &lt;chr&gt;, medal &lt;chr&gt;\n\n\n\n\n\n\n\n\n\n\n\nAs expected there is a fairly strong relationship between height and weight although certainly some outliers exist. Interestingly these outliers tend to be from the USA although there is a bias as there are overall more USA data points than either of the other countries. Other trends that are noticeable are females tend to be lighter and shorter although there is one outlier male that may be the lightest of all the data points. Additionally, china has a larger proportion of of data points that are taller than the rest which was surprising to me as I expected that to not be the case. And of course the Americans take the trophy for heaviest people, furthering the stereotype that Americans are fat even though obviously these are elite athletes.\nThis comes from the tidy tuesday of August the 6th in 2024: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-08-06/readme.md This data specifically comes from RGriffin, it offers a comprehensive historical record of the modern Olympic Games, covering every event from the first Games in Athens in 1896 up to the Rio Games in 2016. The data was collected by the author through web scraping of www.sports-reference.com in May 2018. The R code used for both scraping and cleaning the data is available on GitHub. Itâ€™s highly recommended to review the authorâ€™s kernel before starting your own analysis, as it provides valuable insights and methodologies that can enhance your work with this dataset."
  },
  {
    "objectID": "Obama.html",
    "href": "Obama.html",
    "title": "Obama",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(lubridate)\n\n\nThe data used in this analysis comes from the Obama Presidential Library, specifically from their Digital Research Room. It is publicly available at:\nğŸ”— https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media\nThe dataset is titled tweets.csv, which contains tweets posted from the official @POTUS Twitter account during President Barack Obamaâ€™s administration. The dataset includes columns such as the text of each tweet, timestamp, and other metadata.\nThis analysis focuses primarily on the text column, which contains the actual content of the tweets. The goal is to examine how often President Obama mentioned key policy topics, such as healthcare, climate change, and the economy, over the course of his presidency. The analysis finishes with a study of when Obama tweets, looking for trends and then attempting to put a reason on these trends.\n\n\nCode\nobamaa &lt;- readr::read_csv('tweets.csv')\nprint(obamaa)\n\n\n# A tibble: 27,346 Ã— 10\n   tweet_id in_reply_to_status_id in_reply_to_user_id timestamp     source text \n      &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;\n 1  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Nâ€¦\n 2  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Wâ€¦\n 3  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Tâ€¦\n 4  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"RT â€¦\n 5  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Dâ€¦\n 6  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Wâ€¦\n 7  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"â€œInâ€¦\n 8  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Tâ€¦\n 9  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"â€œI â€¦\n10  7.99e17                    NA                  NA 2016-11-16 1â€¦ \"&lt;a hâ€¦ \"\\\"Pâ€¦\n# â„¹ 27,336 more rows\n# â„¹ 4 more variables: retweeted_status_id &lt;dbl&gt;,\n#   retweeted_status_user_id &lt;dbl&gt;, retweeted_status_timestamp &lt;chr&gt;,\n#   expanded_urls &lt;chr&gt;\n\n\n\n\nCode\ncleanobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),\n         text = str_replace_all(text, \"http[[:alnum:][:punct:]]*\", \"\"),  \n         text = str_replace_all(text, \"[[:punct:]]\", \" \"),\n         text = str_replace_all(text, \"â€”@POTUS.*\", \"\"),\n         text= str_replace_all(text, \"\\n\", \"\")) \n\n\nstopwords &lt;- c(\"the\", \"and\", \"to\", \"of\", \"in\", \"a\", \"on\", \"for\", \"with\", \"is\", \"that\",\"s\", \"at\", \"potus\", \"  \", \"rt\", \"amp\", \"it\",\"this\", \"are\",\"â†’\")\n\n\nword_counts &lt;- cleanobama |&gt;\n  mutate(words = str_split(text, \"\\\\s+\")) |&gt;  \n  unnest(words) |&gt;  \n  filter(words != \"\", !words %in% stopwords) |&gt;  \n  count(words, sort = TRUE)  \n\n\nword_counts |&gt;\n  slice_max(n, n = 10) |&gt; \n  ggplot(aes(x = reorder(words, n), y = n)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Words in Obama's Tweets\",\n       x = \"Word\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThis graph illustrates which words Obama used most throughout his presidency in his tweets while not including â€œtheâ€, â€œandâ€, â€œtoâ€, â€œofâ€, â€œinâ€, â€œaâ€, â€œonâ€, â€œforâ€, â€œwithâ€, â€œisâ€, â€œthatâ€,â€œsâ€, â€œatâ€, â€œpotusâ€, â€œrtâ€, â€œampâ€, â€œitâ€,â€œthisâ€, â€œareâ€ and â€œâ†’â€. This graph is not especially helpful as the most used words are not very descriptive of what is going on in the tweets. What we can learn is that many of his tweets are about current news as he utilizes â€œtodayâ€ heavily and they tend to be directed to a specific audience. By utilizing â€œweâ€ and â€œourâ€ it illustrates that Obama is not just sharing opinions but directing messages to the general public. I do not have a clear explanation why Obama and president are utilized so heavily although I assume that Obama tended to share quotes from himself or articles written which therefore would cite him.\n\n\nCode\npolicy_keywords &lt;- c(\"healthcare\", \"education\", \"climate\", \"economy\", \n                     \"jobs\", \"tax\", \"immigration\", \"gun\", \"poverty\")\n\n\nobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),  # stringr\n         text = str_replace_all(text, \"[[:punct:]]\", \" \"))  #stringr\n\n# regular expressions\nadd_word_boundaries &lt;- function(word) {\n  paste0(\"(?&lt;!\\\\w)\", word, \"(?!\\\\w)\")  #look behind lookaround\n}\n\n\npolicy_regexes &lt;- setNames(map(policy_keywords, add_word_boundaries), policy_keywords)\n\n\npolicy_counts &lt;- map_dfr(names(policy_regexes), ~{\n  keyword_regex &lt;- policy_regexes[[.x]]\n\n\n  obama |&gt;\n    mutate(has_word = str_detect(text, regex(keyword_regex))) |&gt;\n    summarize(policy = .x,\n              count = sum(has_word))  \n}) |&gt;\n  arrange(desc(count))\n\n\nprint(policy_counts)\n\n\n# A tibble: 9 Ã— 2\n  policy      count\n  &lt;chr&gt;       &lt;int&gt;\n1 jobs          953\n2 economy       661\n3 tax           420\n4 education     399\n5 climate       318\n6 gun           272\n7 immigration   226\n8 poverty       144\n9 healthcare     46\n\n\nThe number of times each of the following words is found exactly like this in unique tweets. Eliminates taxpayers and other additions to the words. I decided to create this plot because it highlighted which political themes he addressed most heavily in his tweets and is much more useful than the earlier utilized graph. The top three results are heavily tied together so I am not surprised to see them at the top as I am sure they would all appear together in each unique tweet.\n\n\nCode\nobamatime &lt;- obamaa |&gt;\n  # Split the timestamp into date, time, timezone\n  separate(timestamp, into = c(\"date\", \"time\", \"timezone\"), sep = \"\\\\s+\") |&gt;\n  \n  mutate(time = str_replace_all(time, \"[^0-9:]\", \"\")) |&gt;  \n  \n  # Parse time into an HMS object and create hour and time_chunk columns\n  mutate(tlsime = hms(time),  \n         hour_of_day = hour(tlsime),  \n         time_chunk = cut(hour_of_day, breaks = seq(0, 24, by = 4), \n                          labels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                     \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"), \n                          include.lowest = TRUE)) |&gt; \n  mutate(time_chunk = factor(time_chunk, levels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                                    \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"))) |&gt;  \n  arrange(time_chunk) \n\n# Now count the tweets by time of day\ntweetsbytime &lt;- obamatime |&gt;\n  count(time_chunk)  \n\n\ntweetsbytime |&gt;\n  ggplot(aes(x = time_chunk, y = n)) +\n  geom_col(fill = \"steelblue\") +\n\n  labs(title = \"Obama Tweets by Time of Day (4-hour Chunks)\",\n       x = \"Time of Day (4-hour intervals)\",\n       y = \"Number of Tweets\")\n\n\n\n\n\n\n\n\n\nI then began to work with the timestamp column of the table, first searching for any trends within the time of day in which he would post. I assumed late afternoon as he finished up the structured work day and moved into the later hours of the day in which he may have more freedom to catch up on twitter. This in turn would increase the likelihood that he would see something to repost or share thoughts on issues he had worked with that day. This hypothesis was shown to be correct although I was surprised to see a much larger portion of late night tweets than mid-late morning tweets.\n\n\nCode\nobamatime &lt;- obamatime |&gt;\n  mutate(date = as.Date(date)) \n\n\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month_year = format(date, \"%Y-%m\")) |&gt;  \n  count(month_year)  \n\n\ntweetsbymonth |&gt;\n  ggplot(aes(x = month_year, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month Over His Whole Presidency\",\n       x = \"Month-Year\",\n       y = \"Number of Tweets\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nI was then interested if there were specific times of the year which brought upon extra tweeting from Obama but accidentally did not group by month so I had this messy graph. I was going to delete it but I found this trend quite interesting that the longer he was in office the more tweets he was sending out so I left this even though the x-axis is unclear. That jumble of numbers is the year followed by the month. For example 2012-02.\n\n\nCode\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month = format(date, \"%m\")) |&gt;\n  count(month)  \n\n\ntweetsbymonth |&gt;\n  ggplot(aes(x = month, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month\",\n       x = \"Month\",\n       y = \"Number of Tweets\") +\n  scale_x_discrete(labels = c(\"01\" = \"January\", \"02\" = \"February\", \"03\" = \"March\", \n                              \"04\" = \"April\", \"05\" = \"May\", \"06\" = \"June\",\n                              \"07\" = \"July\", \"08\" = \"August\", \"09\" = \"September\", \n                              \"10\" = \"October\", \"11\" = \"November\", \"12\" = \"December\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nThis is the graph I was actually hoping to create earlier and found that it is much less helpful in showing any real trends. I found it interesting that January had the most tweets and maybe that was due to issuing plans or goals for the new year or possibly sharing information and reports about the inauguration. But what did surprise was that it looked like a ton of tweets were being sent by him so that led me to the following calculation just out of interest.\n\n\nCode\ntotal_tweets &lt;- nrow(obamatime)\n\ntotal_days &lt;- n_distinct(obamatime$date)\n\naverage_tweets_per_day &lt;- total_tweets / total_days\n\n\naverage_tweets_per_day\n\n\n[1] 10.67786\n\n\nThis function found that Obama averaged 10.68 tweets per day which seems pretty ridiculous to me although I do understand that often he was just reposting something that had been created by others. I also understand this is supposed to be an informal type of sharing information so these may be quick thoughts that he quickly types up and sends out. Lastly, it would not be surprising if there are other individuals who post for him so many of the tweets may not be manually handed by Obama."
  },
  {
    "objectID": "simulationproject.html",
    "href": "simulationproject.html",
    "title": "Will I ever get a single??",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n  # Simulate the process until all students have picked a room\n  while (sum(students_picked) &lt; total_students) {\n    # Randomly select 5 students who haven't picked a room yet\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    # Mark those students as having picked a room\n    students_picked[selected_students] &lt;- TRUE\n    \n    # Add selected students to the order of picking\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    # Increment the number of draws (or rounds)\n    num_draws &lt;- num_draws + 1\n  }\n  \n  # Return the order of picking and the number of draws\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n# Run the simulation 3 times\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n# Set up a plot\nggplot() \n\n\n\n\n\n\n\n\n\nCode\n  # Plot for each simulation\n  map2(simulations, 1:3, ~ {\n    # Get the order of picking for this simulation\n    order_of_picking &lt;- .x$order_of_picking\n    draw_number &lt;- 1:length(order_of_picking)\n    \n    # Determine the color for each student (red for the first 70, blue otherwise)\n    colors &lt;- ifelse(order_of_picking &lt;= 70, \"red\", \"steelblue\")\n    \n    # Create a tibble to plot the points\n    tibble(student = order_of_picking, draw_number = draw_number, color = colors) %&gt;%\n      ggplot(aes(x = draw_number, y = student, color = color)) +\n      geom_point() +\n      labs(title = paste(\"Room Selection Simulation\", .y), x = \"Draw Number (Time Interval)\", y = \"Student ID\") +\n      theme_minimal() +\n      scale_color_identity() + \n      theme(legend.position = \"none\")\n  })\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n  # Simulate the process until all students have picked a room\n  while (sum(students_picked) &lt; total_students) {\n    # Randomly select 5 students who haven't picked a room yet\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    # Mark those students as having picked a room\n    students_picked[selected_students] &lt;- TRUE\n    \n    # Add selected students to the order of picking\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    # Increment the number of draws (or rounds)\n    num_draws &lt;- num_draws + 1\n  }\n  \n  # Return the order of picking and the number of draws\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n# Run the simulation 3 times\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n# Combine all the students who were red dots in each simulation\nred_dot_students &lt;- map(simulations, ~ .x$order_of_picking[1:70])\n\n# Unify all the red dot students from all simulations\nall_red_dots &lt;- unique(unlist(red_dot_students))\n\n# Calculate the probability of being a red dot at least once\ntotal_students &lt;- 400\nprob_red_dot_at_least_once &lt;- length(all_red_dots) / total_students\nprob_red_dot_at_least_once\n\n\n[1] 0.4375"
  }
]