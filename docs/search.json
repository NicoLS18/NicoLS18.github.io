[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! Coming soon."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas Laub-Sabater",
    "section": "",
    "text": "Hello! My name is Nicolas Laub-Sabater and I am an economics and environmental science double major at Pomona College. I enjoy fishing, competing in different sports such as golf and in general just spending time outside. I am also Puerto Rican which has given me the opportunity to be bilingual and I have continued to cherish this gift and hope to improve it when I go abroad to Spain next semester!"
  },
  {
    "objectID": "tt2.html",
    "href": "tt2.html",
    "title": "GPT Detectors",
    "section": "",
    "text": "This data was collected in an experiment to track how effective AI detectors were at truly capturing AI created work and whether non-native english speakers were at a higher likelihood to be flagged as AI. I chose to focus solely on the effectiveness of the AI detectors as I have always assumed that they are quite good at distinguishing work that is done 100% by a human vs an AI.\n\n\nCode\ndf&lt;- detectors\nmatch_percentage_by_model &lt;- detectors |&gt;\n  \n  group_by(detector) |&gt;\n  \n#statistically analyzing the data\n  summarise(match_percentage = mean(kind == .pred_class, na.rm = TRUE) * 100)\n\n\nprint(match_percentage_by_model)\n\n\n# A tibble: 7 × 2\n  detector      match_percentage\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Crossplag                 50.1\n2 GPTZero                   48.9\n3 HFOpenAI                  51.4\n4 OriginalityAI             59.0\n5 Quil                      47.8\n6 Sapling                   50  \n7 ZeroGPT                   51.7\n\n\nCode\n#graph\nggplot(match_percentage_by_model, aes(x = detector, y = match_percentage, fill = detector)) +\n  geom_bar(stat = \"identity\") +  # Bar plot\n  theme_minimal() +  # Minimal theme\n  labs(\n    title = \"Match Percentage by AI Model\",\n    x = \"AI Model\",\n    y = \"Match Percentage\"\n  )\n\n\n\n\n\n\n\n\n\nIt is interesting to see that these AI detectors are not very good at all. They are supposed to be quite good, especially at detecting when something is written 100% by AI or human. It urges the question of which side is at fault, has AI writing just advanced so much or is AI detecting simply not a very strong technology at the current moment. I have unfortunately missed the true goal of the data set but my initial interest was focused on the true capabilities of this AI detection and in my additional graphs I would focus more on the native/non-native piece of this data.\nThis information comes from the Tidy Tuesday of July 18th 2023: https://github.com/rfordatascience/tidytuesday/tree/main/data/2023/2023-07-18\nThis dataset, created by Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou, was designed to address their hypothesis that GPT detectors exhibit bias against non-native English writers. The authors aim to investigate the fairness and effectiveness of widely-used GPT detectors in distinguishing between AI-generated and human-written content. With the growing reliance on generative language models, the researchers recognize the potential for misuse and are concerned about the impact on non-native English speakers. Through evaluating GPT detectors using writing samples from both native and non-native English writers, they discovered a pattern of misclassification, where non-native English samples were often incorrectly flagged as AI-generated, while native samples were accurately identified. The study also found that simple prompting strategies could reduce this bias and bypass detectors, further highlighting the unintentional penalization of writers with limited linguistic resources. Their findings emphasize the ethical considerations of using such detectors, particularly in evaluative or educational contexts, and raise awareness about the potential exclusion of non-native English speakers from global conversations. https://arxiv.org/abs/2304.02819"
  },
  {
    "objectID": "tidytuesday.html",
    "href": "tidytuesday.html",
    "title": "Bob’s Burgers",
    "section": "",
    "text": "tuesdata &lt;- tidytuesdayR::tt_load('2024-11-19') \n\n---- Compiling #TidyTuesday Information for 2024-11-19 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"episode_metrics.csv\"\n\nepisode_metrics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-11-19/episode_metrics.csv')\n\nRows: 272 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (8): season, episode, dialogue_density, avg_length, sentiment_variance, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "BobsBurger.html",
    "href": "BobsBurger.html",
    "title": "Olympics",
    "section": "",
    "text": "---- Compiling #TidyTuesday Information for 2024-08-06 ----\n--- There is 1 file available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 1: \"olympics.csv\"\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "Olympics.html",
    "href": "Olympics.html",
    "title": "Olympics: Height and Weight",
    "section": "",
    "text": "This is a historical dataset on the modern Olympic Games, including all the Games from Athens 1896 to Rio 2016. I scraped this data from www.sports-reference.com in May 2018. It includes categories such as team, sport, height and weight. I have decided to take a look at the physical aspects of these athletes to see if there are any differences depending on where they come from around the globe.\n\n\n# A tibble: 271,116 × 15\n      id name     sex     age height weight team  noc   games  year season city \n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1     1 A Dijia… M        24    180     80 China CHN   1992…  1992 Summer Barc…\n 2     2 A Lamusi M        23    170     60 China CHN   2012…  2012 Summer Lond…\n 3     3 Gunnar … M        24     NA     NA Denm… DEN   1920…  1920 Summer Antw…\n 4     4 Edgar L… M        34     NA     NA Denm… DEN   1900…  1900 Summer Paris\n 5     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 6     5 Christi… F        21    185     82 Neth… NED   1988…  1988 Winter Calg…\n 7     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 8     5 Christi… F        25    185     82 Neth… NED   1992…  1992 Winter Albe…\n 9     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n10     5 Christi… F        27    185     82 Neth… NED   1994…  1994 Winter Lill…\n# ℹ 271,106 more rows\n# ℹ 3 more variables: sport &lt;chr&gt;, event &lt;chr&gt;, medal &lt;chr&gt;\n\n\n\n\nCode\nsubset_olympics &lt;- olympics |&gt; \n  dplyr::filter(team %in% c(\"United States\", \"China\", \"Netherlands\"))\nggplot(subset_olympics, aes(x=height, y=weight, color=team, shape=sex))+\ngeom_point(alpha=0.7)\n\n\n\n\n\n\n\n\n\nAs expected there is a fairly strong relationship between height and weight although certainly some outliers exist. Interestingly these outliers tend to be from the USA although there is a bias as there are overall more USA data points than either of the other countries. Other trends that are noticeable are females tend to be lighter and shorter although there is one outlier male that may be the lightest of all the data points. Additionally, china has a larger proportion of of data points that are taller than the rest which was surprising to me as I expected that to not be the case. And of course the Americans take the trophy for heaviest people, furthering the stereotype that Americans are fat even though obviously these are elite athletes.\nThis comes from the tidy tuesday of August the 6th in 2024: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-08-06/readme.md This data specifically comes from RGriffin, it offers a comprehensive historical record of the modern Olympic Games, covering every event from the first Games in Athens in 1896 up to the Rio Games in 2016. The data was collected by the author through web scraping of www.sports-reference.com in May 2018. The R code used for both scraping and cleaning the data is available on GitHub. It’s highly recommended to review the author’s kernel before starting your own analysis, as it provides valuable insights and methodologies that can enhance your work with this dataset."
  },
  {
    "objectID": "Obama.html",
    "href": "Obama.html",
    "title": "Obama",
    "section": "",
    "text": "Code\n#libraries\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(lubridate)\n\n\nThe data used in this analysis comes from the Obama Presidential Library, specifically from their Digital Research Room. It is publicly available at:\n🔗 https://www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media\nThe dataset is titled tweets.csv, which contains tweets posted from the official @POTUS Twitter account during President Barack Obama’s administration. The dataset includes columns such as the text of each tweet, timestamp, and other metadata.\nThis analysis focuses primarily on the text column, which contains the actual content of the tweets. The goal is to examine how often President Obama mentioned key policy topics, such as healthcare, climate change, and the economy, over the course of his presidency. The analysis finishes with a study of when Obama tweets, looking for trends and then attempting to put a reason on these trends.\n\n\nCode\n#reading in data\nobamaa &lt;- readr::read_csv('tweets.csv')\nhead(obamaa)\n\n\n# A tibble: 6 × 10\n  tweet_id in_reply_to_status_id in_reply_to_user_id timestamp      source text \n     &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;\n1  7.99e17                    NA                  NA 2016-11-16 15… \"&lt;a h… \"\\\"N…\n2  7.99e17                    NA                  NA 2016-11-16 15… \"&lt;a h… \"\\\"W…\n3  7.99e17                    NA                  NA 2016-11-16 15… \"&lt;a h… \"\\\"T…\n4  7.99e17                    NA                  NA 2016-11-16 15… \"&lt;a h… \"RT …\n5  7.99e17                    NA                  NA 2016-11-16 14… \"&lt;a h… \"\\\"D…\n6  7.99e17                    NA                  NA 2016-11-16 14… \"&lt;a h… \"\\\"W…\n# ℹ 4 more variables: retweeted_status_id &lt;dbl&gt;,\n#   retweeted_status_user_id &lt;dbl&gt;, retweeted_status_timestamp &lt;chr&gt;,\n#   expanded_urls &lt;chr&gt;\n\n\n\n\nCode\n#cleaning the data for analysis\ncleanobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),\n         text = str_replace_all(text, \"http[[:alnum:][:punct:]]*\", \"\"),  \n         text = str_replace_all(text, \"[[:punct:]]\", \" \"),\n         text = str_replace_all(text, \"—@POTUS.*\", \"\"),\n         text= str_replace_all(text, \"\\n\", \"\")) \n\n#what moves to remove from the analysis\nstopwords &lt;- c(\"the\", \"and\", \"to\", \"of\", \"in\", \"a\", \"on\", \"for\", \"with\", \"is\", \"that\",\"s\", \"at\", \"potus\", \"  \", \"rt\", \"amp\", \"it\",\"this\", \"are\",\"→\")\n\n#filtering out stopwords and counting the words\nword_counts &lt;- cleanobama |&gt;\n  mutate(words = str_split(text, \"\\\\s+\")) |&gt;  \n  unnest(words) |&gt;  \n  filter(words != \"\", !words %in% stopwords) |&gt;  \n  count(words, sort = TRUE)  \n\n#creating the graph \nword_counts |&gt;\n  slice_max(n, n = 10) |&gt; \n  ggplot(aes(x = reorder(words, n), y = n)) +\n  geom_col(fill = \"skyblue\") +\n  coord_flip() +\n  labs(title = \"Top 10 Words in Obama's Tweets\",\n       x = \"Word\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nThis graph illustrates which words Obama used most throughout his presidency in his tweets while not including “the”, “and”, “to”, “of”, “in”, “a”, “on”, “for”, “with”, “is”, “that”,“s”, “at”, “potus”, “rt”, “amp”, “it”,“this”, “are” and “→”. This graph is not especially helpful as the most used words are not very descriptive of what is going on in the tweets. What we can learn is that many of his tweets are about current news as he utilizes “today” heavily and they tend to be directed to a specific audience. By utilizing “we” and “our” it illustrates that Obama is not just sharing opinions but directing messages to the general public. I do not have a clear explanation why Obama and president are utilized so heavily although I assume that Obama tended to share quotes from himself or articles written which therefore would cite him.\n\n\nCode\n#words to focus on \npolicy_keywords &lt;- c(\"healthcare\", \"education\", \"climate\", \"economy\", \n                     \"jobs\", \"tax\", \"immigration\", \"gun\", \"poverty\")\n\n\nobama &lt;- obamaa |&gt;\n  mutate(text = str_to_lower(text),  # stringr\n         text = str_replace_all(text, \"[[:punct:]]\", \" \"))  #stringr\n\n# regular expressions\nadd_word_boundaries &lt;- function(word) {\n  paste0(\"(?&lt;!\\\\w)\", word, \"(?!\\\\w)\")  #look behind lookaround\n}\n\n\npolicy_regexes &lt;- setNames(map(policy_keywords, add_word_boundaries), policy_keywords)\n\n\npolicy_counts &lt;- map_dfr(names(policy_regexes), ~{\n  keyword_regex &lt;- policy_regexes[[.x]]\n\n\n  obama |&gt;\n    mutate(has_word = str_detect(text, regex(keyword_regex))) |&gt;\n    summarize(policy = .x,\n              count = sum(has_word))  \n}) |&gt;\n  arrange(desc(count))\n\n\nprint(policy_counts)\n\n\n# A tibble: 9 × 2\n  policy      count\n  &lt;chr&gt;       &lt;int&gt;\n1 jobs          953\n2 economy       661\n3 tax           420\n4 education     399\n5 climate       318\n6 gun           272\n7 immigration   226\n8 poverty       144\n9 healthcare     46\n\n\nThe number of times each of the following words is found exactly like this in unique tweets. Eliminates taxpayers and other additions to the words. I decided to create this plot because it highlighted which political themes he addressed most heavily in his tweets and is much more useful than the earlier utilized graph. The top three results are heavily tied together so I am not surprised to see them at the top as I am sure they would all appear together in each unique tweet.\n\n\nCode\nobamatime &lt;- obamaa |&gt;\n  # Split the timestamp into date, time, timezone\n  separate(timestamp, into = c(\"date\", \"time\", \"timezone\"), sep = \"\\\\s+\") |&gt;\n  \n  mutate(time = str_replace_all(time, \"[^0-9:]\", \"\")) |&gt;  \n  \n  # Turn time into an HMS object and create hour and time_chunk columns\n  mutate(tlsime = hms(time),  \n         hour_of_day = hour(tlsime),  \n         time_chunk = cut(hour_of_day, breaks = seq(0, 24, by = 4), \n                          labels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                     \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"), \n                          include.lowest = TRUE)) |&gt; \n  mutate(time_chunk = factor(time_chunk, levels = c(\"00:00-03:59\", \"04:00-07:59\", \"08:00-11:59\", \n                                                    \"12:00-15:59\", \"16:00-19:59\", \"20:00-23:59\"))) |&gt;  \n  arrange(time_chunk) \n\n# Now count the tweets by time of day\ntweetsbytime &lt;- obamatime |&gt;\n  count(time_chunk)  \n\n#graph\ntweetsbytime |&gt;\n  ggplot(aes(x = time_chunk, y = n)) +\n  geom_col(fill = \"steelblue\") +\n\n  labs(title = \"Obama Tweets by Time of Day (4-hour Chunks)\",\n       x = \"Time of Day (4-hour intervals)\",\n       y = \"Number of Tweets\")\n\n\n\n\n\n\n\n\n\nI then began to work with the timestamp column of the table, first searching for any trends within the time of day in which he would post. I assumed late afternoon as he finished up the structured work day and moved into the later hours of the day in which he may have more freedom to catch up on twitter. This in turn would increase the likelihood that he would see something to repost or share thoughts on issues he had worked with that day. This hypothesis was shown to be correct although I was surprised to see a much larger portion of late night tweets than mid-late morning tweets.\n\n\nCode\n#convert to time format\nobamatime &lt;- obamatime |&gt;\n  mutate(date = as.Date(date)) \n\n\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month_year = format(date, \"%Y-%m\")) |&gt;  \n  count(month_year)  \n\n#graph\ntweetsbymonth |&gt;\n  ggplot(aes(x = month_year, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month Over His Whole Presidency\",\n       x = \"Month-Year\",\n       y = \"Number of Tweets\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nI was then interested if there were specific times of the year which brought upon extra tweeting from Obama but accidentally did not group by month so I had this messy graph. I was going to delete it but I found this trend quite interesting that the longer he was in office the more tweets he was sending out so I left this even though the x-axis is unclear. That jumble of numbers is the year followed by the month. For example 2012-02.\n\n\nCode\n#convert to time format\ntweetsbymonth &lt;- obamatime |&gt;\n  mutate(month = format(date, \"%m\")) |&gt;\n  count(month)  \n\n#graph\ntweetsbymonth |&gt;\n  ggplot(aes(x = month, y = n)) +\n  geom_col(fill = \"steelblue\") +\n  labs(title = \"Obama Tweets by Month\",\n       x = \"Month\",\n       y = \"Number of Tweets\") +\n  scale_x_discrete(labels = c(\"01\" = \"January\", \"02\" = \"February\", \"03\" = \"March\", \n                              \"04\" = \"April\", \"05\" = \"May\", \"06\" = \"June\",\n                              \"07\" = \"July\", \"08\" = \"August\", \"09\" = \"September\", \n                              \"10\" = \"October\", \"11\" = \"November\", \"12\" = \"December\")) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  \n\n\n\n\n\n\n\n\n\nThis is the graph I was actually hoping to create earlier and found that it is much less helpful in showing any real trends. I found it interesting that January had the most tweets and maybe that was due to issuing plans or goals for the new year or possibly sharing information and reports about the inauguration. But what did surprise was that it looked like a ton of tweets were being sent by him so that led me to the following calculation just out of interest.\n\n\nCode\ntotal_tweets &lt;- nrow(obamatime)\n\ntotal_days &lt;- n_distinct(obamatime$date)\n\naverage_tweets_per_day &lt;- total_tweets / total_days\n\n\naverage_tweets_per_day\n\n\n[1] 10.67786\n\n\nThis function found that Obama averaged 10.68 tweets per day which seems pretty ridiculous to me although I do understand that often he was just reposting something that had been created by others. I also understand this is supposed to be an informal type of sharing information so these may be quick thoughts that he quickly types up and sends out. Lastly, it would not be surprising if there are other individuals who post for him so many of the tweets may not be manually handed by Obama."
  },
  {
    "objectID": "simulationproject.html",
    "href": "simulationproject.html",
    "title": "Will I ever get a single??",
    "section": "",
    "text": "Last semester I was given one of the worst possible times, ending up in the last 15 mins of the nearly 4 hour process. My aspirations of getting a single dashed in an instant when I checked my time. Now as I look ahead for the following years I am still looking to occupy my own room. I understand there are now more options such as a suite or going off campus but in order to keep the simulation effective and repeatable with map I utilized the same concept for all three years. (Freshman year there was no room draw). I estimated that there were around 70 singles opportunities for the 400 students we have in each grade. I then simulated each and then calculated the percentage of students that get a a single at least once in their three years here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n \n  students_picked &lt;- rep(FALSE, total_students)\n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    students_picked[selected_students] &lt;- TRUE\n    \n  \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    \n    num_draws &lt;- num_draws + 1\n  }\n  \n \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\n  map2(simulations, 1:3, ~ {\n    \n    order_of_picking &lt;- .x$order_of_picking\n    draw_number &lt;- 1:length(order_of_picking)\n    \n    \n    colors &lt;- ifelse(order_of_picking &lt;= 70, \"red\", \"blue\")\n    \n   \n    tibble(student = order_of_picking, draw_number = draw_number, color = colors) %&gt;%\n      ggplot(aes(x = draw_number, y = student, color = color)) +\n      geom_point() +\n      labs(title = paste(\"Room Selection Simulation\", .y), x = \"Draw Number (Time Interval)\", y = \"Student ID\") +\n      theme_minimal() +\n      scale_color_identity() + \n      theme(legend.position = \"none\")\n  })\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n \n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n\n  while (sum(students_picked) &lt; total_students) {\n \n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n\n    students_picked[selected_students] &lt;- TRUE\n    \n    \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n  \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nred_dot_students &lt;- map(simulations, ~ .x$order_of_picking[1:70])\n\n\nall_red_dots &lt;- unique(unlist(red_dot_students))\n\n\ntotal_students &lt;- 400\nprob_red_dot_at_least_once &lt;- length(all_red_dots) / total_students\nprob_red_dot_at_least_once\n\n\n[1] 0.4375\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n  \n    students_picked[selected_students] &lt;- TRUE\n    \n \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n   \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nstudent_red_dot_count &lt;- rep(0, 400)\n\n\n#utilized walk instead of map here bc I found that it would eliminate the result and would set up the graph without teh additional numbers. \nwalk(simulations, ~ {\n  red_dot_students &lt;- .x$order_of_picking[1:70]\n  student_red_dot_count[red_dot_students] &lt;&lt;- student_red_dot_count[red_dot_students] + 1\n})\n\n\ndf &lt;- tibble(red_dot_count = student_red_dot_count)\n\n\n\nggplot(df, aes(x = red_dot_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Distribution of How Many Times Students Got a Single\",\n       x = \"Times selected for Single\",\n       y = \"Number of Students\") +\n  scale_x_continuous(breaks = 0:3) +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCreated the simulation and ran it 3 times which can be seen in the first three scatter plots although those are not very helpful as it impossible to identify which student ID is which. So then I calculated the overall percentage of someone receiving at least one single throughout the 3 years and it came out to 43%. Then in order to create a more effective visual representation I created a histogram that shows how many times each student got a single. This showed there is even a chance for a student to get a time early enough for a single all 3 room draws. It also showed that there is a large portion of the student body that will never get a good room draw and honestly I think they should change the process a little so that if you have a terrible time the first room draw you should be guaranteed in the first half, much like the lottery for picking classes. With this new information that I have a 43%* chance of getting a single at least once in my three years of room draws I suspect I will be a part of the 57%. It will also all come down to whether study abroad messes up my chances or improves my chances… We shall see."
  },
  {
    "objectID": "roomselection.html",
    "href": "roomselection.html",
    "title": "roomselection",
    "section": "",
    "text": "Last semester I was given one of the worst possible times, ending up in the last 15 mins of the nearly 4 hour process. My aspirations of getting a single dashed in an instant when I checked my time. Now as I look ahead for the following years I am still looking to occupy my own room. I understand there are now more options such as a suite or going off campus but in order to keep the simulation effective and repeatable with map I utilized the same concept for all three years. (Freshman year there was no room draw). I estimated that there were around 70 singles opportunities for the 400 students we have in each grade. I then simulated each and then calculated the percentage of students that get a a single at least once in their three years here.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n  # Simulate the process until all students have picked a room\n  while (sum(students_picked) &lt; total_students) {\n    # Randomly select 5 students who haven't picked a room yet\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n    # Mark those students as having picked a room\n    students_picked[selected_students] &lt;- TRUE\n    \n    # Add selected students to the order of picking\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n    # Increment the number of draws (or rounds)\n    num_draws &lt;- num_draws + 1\n  }\n  \n  # Return the order of picking and the number of draws\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\n\n  # Plot for each simulation\n  map2(simulations, 1:3, ~ {\n    # Get the order of picking for this simulation\n    order_of_picking &lt;- .x$order_of_picking\n    draw_number &lt;- 1:length(order_of_picking)\n    \n    # Determine the color for each student (red for the first 70, blue otherwise)\n    colors &lt;- ifelse(order_of_picking &lt;= 70, \"red\", \"blue\")\n    \n    # Create a tibble to plot the points\n    tibble(student = order_of_picking, draw_number = draw_number, color = colors) %&gt;%\n      ggplot(aes(x = draw_number, y = student, color = color)) +\n      geom_point() +\n      labs(title = paste(\"Room Selection Simulation\", .y), x = \"Draw Number (Time Interval)\", y = \"Student ID\") +\n      theme_minimal() +\n      scale_color_identity() + \n      theme(legend.position = \"none\")\n  })\n\n\n[[1]]\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Function to simulate the room draw\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n  # Create a vector to represent students (1 to 400)\n  students &lt;- 1:total_students\n  \n  # Create a vector to track students who have picked a room\n  students_picked &lt;- rep(FALSE, total_students)\n  \n  # Vector to track the order of students picking a room\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n\n    students_picked[selected_students] &lt;- TRUE\n    \n\n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n\n    num_draws &lt;- num_draws + 1\n  }\n  \n\n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nred_dot_students &lt;- map(simulations, ~ .x$order_of_picking[1:70])\n\n\nall_red_dots &lt;- unique(unlist(red_dot_students))\n\n\ntotal_students &lt;- 400\nprob_red_dot_at_least_once &lt;- length(all_red_dots) / total_students\nprob_red_dot_at_least_once\n\n\n[1] 0.4375\n\n\n\n\nCode\nsimulate_room_draw &lt;- function(total_students = 400, students_per_draw = 5, draw_interval = 3) {\n\n  students &lt;- 1:total_students\n  \n\n  students_picked &lt;- rep(FALSE, total_students)\n  \n\n  order_of_picking &lt;- c()\n  \n  num_draws &lt;- 0\n  \n\n  while (sum(students_picked) &lt; total_students) {\n\n    available_students &lt;- which(!students_picked)\n    selected_students &lt;- sample(available_students, min(students_per_draw, length(available_students)), replace = FALSE)\n    \n  \n    students_picked[selected_students] &lt;- TRUE\n    \n \n    order_of_picking &lt;- c(order_of_picking, selected_students)\n    \n   \n    num_draws &lt;- num_draws + 1\n  }\n  \n  \n  list(order_of_picking = order_of_picking, num_draws = num_draws)\n}\n\n\nset.seed(42)\nsimulations &lt;- map(1:3, ~ simulate_room_draw())\n\n\nstudent_red_dot_count &lt;- rep(0, 400)\n\n#utilized walk instead of map here bc I found that it would eliminate the result and would set up the graph without teh additional numbers. \nwalk(simulations, ~ {\n  red_dot_students &lt;- .x$order_of_picking[1:70]\n  student_red_dot_count[red_dot_students] &lt;&lt;- student_red_dot_count[red_dot_students] + 1\n})\n\n\ndf &lt;- tibble(red_dot_count = student_red_dot_count)\n\n\n\nggplot(df, aes(x = red_dot_count)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Distribution of How Many Times Students Got a Single\",\n       x = \"Times selected for Single\",\n       y = \"Number of Students\") +\n  scale_x_continuous(breaks = 0:3) +  \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCreated the simulation and ran it 3 times which can be seen in the first three scatter plots although those are not very helpful as it impossible to identify which student ID is which. So then I calculated the overall percentage of someone receiving at least one single throughout the 3 years and it came out to 43%. Then in order to create a more effective visual representation I created a histogram that shows how many times each student got a single."
  },
  {
    "objectID": "ethical.html",
    "href": "ethical.html",
    "title": "Ethics within Data Science",
    "section": "",
    "text": "04/16/2025\nData Science Ethics \n\n\nHow can data science exacerbate current inequalities and what can be done to rely on AI but also ensure it is not inadvertently creating a bias. \n\n\nAmazon’s now-discontinued AI recruiting tool serves as a powerful example of the unintended consequences AI can have when left unchecked. Designed to streamline and optimize the hiring process, the tool was trained on resumes submitted over a ten-year period—most of which came from men. As a result, it developed a bias against female candidates, downgrading resumes that included terms like “women’s” or references to all-women’s colleges. While the intention behind the program was positive, and it likely would have identified strong candidates, its flawed data input led it to replicate and even amplify existing gender disparities. This issue is especially relevant today: while data anonymity and security are heavily discussed and increasingly regulated, there is still a lack of clear frameworks for ensuring AI is used responsibly. The real challenge lies in holding companies accountable for the societal impacts of their AI systems—particularly when those impacts are subtle, systemic, and not immediately visible.\nThis is an important lesson, as the biases that are found in the data that we feed the program will inevitably be reproduced by the program as it attempts to match what it has been given. I think an understanding of this concept will be important for preventing future issues but still testing the results of the AI algorithms is crucial to prevent any unintentional harm produced by the AI. This overall issue and the societal impacts of these AI programs fall under the following ethical dilemmas: Consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society, Recognize and mitigate bias in ourselves and in the data we use. In the Amazon example there was a failure to see the bias in the data they were using which in turn led to an ethical dilemma of how they used the data; another example is one I found on MIT’s tech review page about the use of crime predictive systems.\nPredictive policing algorithms, like PredPol, have been criticized for perpetuating systemic racism by relying on biased data sources. Even when developers attempt to reduce bias by using victim reports instead of arrest records, these tools still disproportionately target Black and minority communities. This occurs because victim reports themselves can be influenced by societal biases and disparities in reporting practices. For instance, wealthier white individuals are more likely to report crimes committed by poorer Black individuals, and Black individuals are also more likely to report other Black individuals. These patterns lead algorithms to over-police certain neighborhoods, creating a feedback loop that reinforces existing inequalities. Efforts to adjust these models have shown limited success, highlighting the challenge of mitigating bias in predictive policing. The core issue lies not just in the data but in the structural and social conditions that skew crime data, making it difficult for algorithms to provide fair and accurate predictions.\nSimilarly this program is utilizing biases for its learning which in turn leads to biases, and as this article pointed out these new biases created a feedback loop. This is a system that is still utilized and again should really be considered more thoughtfully as the impacts this system has on individuals in society is extreme and it promotes the very biases that much of society is actively trying to tear down. \n\n\n\nWho was measured? Are those individuals representative of the people to whom we’d like to generalize / apply the algorithm?\n\n\nAmazon AI Hiring Tool: \nThe individuals measured by Amazon’s AI hiring tool were applicants who had previously submitted resumes over a ten-year span, the majority of whom were men already hired by Amazon. These individuals became the data foundation for the algorithm, meaning the system essentially learned from and attempted to replicate the profile of past hires. However, this group is not representative of the broader population of talented job seekers Amazon might want to consider today, especially women and other underrepresented groups. The tool assumed the historical data reflected ideal hiring outcomes, without recognizing that past decisions might have been influenced by implicit biases or systemic exclusions. As a result, the algorithm favored male-associated language and penalized terms like “women’s,” leading to a skewed view of what constitutes a strong candidate. This misalignment between the training data and the intended applicant pool reveals how even well-intentioned AI can perpetuate inequality if the data used to train it isn’t critically evaluated.\nPredictive Policing Algorithms:\nIn the case of predictive policing tools like those developed by Geolitica (formerly PredPol), the individuals measured are those who have historically been arrested or involved in recorded crime incidents—data that disproportionately comes from heavily policed, often low-income communities of color. These individuals do not represent the total population likely to commit crimes, nor do they reflect a fair cross-section of neighborhoods or demographic groups. Instead, they reflect the biases of decades of discriminatory policing practices. The algorithm then reinforces these patterns, continuously sending law enforcement back into the same areas because of a feedback loop: more police presence leads to more recorded incidents, which justifies further surveillance. This results in a misapplication of the tool, where the very people it targets are not a neutral or complete representation of crime trends but rather a reflection of systemic bias embedded in the data.\n\n\nRecognize and mitigate bias in ourselves and in the data we use:\nThe ethical principle of recognizing and mitigating bias is clearly illustrated in both the Amazon AI hiring tool and predictive policing algorithms, where the underlying data reflected long-standing inequalities. In Amazon’s case, the data came from a predominantly male hiring history, which led the algorithm to develop a preference for resumes resembling those of men—penalizing female-associated terms in the process. Although the system itself was technically proficient, it unknowingly mirrored the company’s past biases, reinforcing gender disparities rather than removing them. Similarly, predictive policing tools rely on historical crime data shaped by years of racially biased policing. These algorithms don’t just inherit that bias—they amplify it, creating cycles of over-policing in marginalized communities. These cases emphasize that bias isn’t just a flaw in datasets, but often stems from unchallenged human assumptions. Recognizing this, and actively working to question and correct both our own blind spots and the flaws in the data we rely on, is essential for building AI systems that serve everyone fairly.\n\n\n\nConsider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society.\nAmazon AI Hiring Tool:\nThe ethical implications of Amazon’s AI hiring tool highlight the importance of scrutinizing how data choices can shape real-world outcomes. At face value, automating the hiring process promised efficiency and objectivity, but in reality, it reproduced and institutionalized gender bias present in the company’s past hiring patterns. The decision to train the algorithm on resumes of previously successful applicants ignored the possibility that historical decisions might have excluded qualified women. This oversight had tangible consequences: women’s resumes were systematically rated lower, reinforcing a biased notion of what a “qualified” applicant looks like. This example demonstrates how even subtle design choices—like which data to include or which features to emphasize—can lead to ethical missteps that affect individual opportunities and contribute to broader patterns of exclusion in the workforce.\nPredictive Policing Algorithms:\nThe use of predictive policing algorithms underscores how data-driven decisions can deeply affect communities and reinforce systemic injustice. By basing predictions on historical arrest data, these systems essentially codify decades of racially biased policing into digital form. The impact goes beyond inaccurate forecasts—it determines where police are sent, who is surveilled, and ultimately who is criminalized. The ethical failure lies in treating biased data as objective truth, without accounting for the societal structures that shaped that data. This kind of technological decision-making not only affects individuals who are unfairly targeted but also erodes trust in public institutions and perpetuates a cycle of marginalization. It’s a stark reminder that the choices we make when building and deploying data systems have far-reaching consequences that extend well beyond technical performance.\n\n\n\nBe open to changing our methods and conclusions in response to new knowledge.\nThe value of being open to changing methods and conclusions in response to new knowledge is exemplified by Amazon’s decision to shut down its AI hiring tool after recognizing its embedded gender bias. Once the company identified that the system was unfairly penalizing resumes containing female-associated terms, they chose not to move forward with its deployment—even though the tool likely showed strong performance in other areas. This decision reflects a willingness to confront uncomfortable truths, reevaluate their approach, and prioritize fairness over automation. In contrast, predictive policing programs have continued to operate despite mounting evidence that they disproportionately target marginalized communities and reinforce systemic racism. Rather than pause or reconsider their deployment, many law enforcement agencies have persisted in using these tools, often citing their efficiency while overlooking the deep social costs. This contrast highlights the ethical divide between organizations willing to evolve in light of new understanding and those that continue harmful practices under the guise of technological progress.\n\n\n\n\n\nAfter writing the above content my mom came across another article related to these concepts which I thought was exceptionally intriguing so I have decided to add an additional couple paragraphs addressing a few of the earlier questions/values but with the new situation. The new example I am focusing on is the facial recognition AI program by Clearview AI. Clearview scraped billions of photos off of social media platforms without the consent of the platforms or the individuals and was actually fined tens of millions of dollars by multiple countries for disregarding privacy laws while creating their database. So clearly when looking at whether this data should be utilized it absolutely should not as there has been no consent to its acquisition and therefore its use. Yet the USA instead of imposing fines or attempting to restrict Clearview has implemented their technology in major branches of government. Similarly to predictive policing, this data is mainly utilized in law enforcement agencies, specifically ICE and the FBI. Also similarly, this data is mainly utilized to surveil marginalized populations (Business & Human Rights Resource Centre) along the borders and within the United States. \nThere are a couple of clear issues with this technology, beginning with the concept of over policing a certain demographic will lead to more violations, which leads to more policing and overall the same feedback loop found in predictive policing. This feedback loop would further promote negative stereotypes of migrants entering the US, which may be the goal of ICE as they would in turn receive more funding. Then the most disturbing issue being the fact that all of their data was taken without any consent by the individuals or the platforms that hosted them. Instead Clearview will not say where they accessed these photos although clearly they came from social media platforms. Clearview as discussed earlier has been criticized by other countries in a major way and yet the USA is instead supporting them which I find surprising and is a scary look for how the future of data privacy protection legislation may unfold. Currently privacy laws have been mainly designated by state especially because with so much political turmoil many large bills that have been introduced have been unable to pass. This is also due to the “increasing complexity of privacy concerns” (DLA Piper) which has made a comprehensive act essentially impossible to impose. Overall these privacy concerns has been pushed to the side and any efforts are quickly shut down so we may not be seeing any truly effective and comprehensive privacy legislation being implemented any time soon.\n\n\nIn conclusion maybe I should have discussed data collection because I had a general assumption that legislation was being slowly implemented to prevent programs such as this but I am clearly wrong. Illegal data collection will continue to be an enormous issue especially if the government financially supports it and puts millions of dollars into their contracts. \n\n\n\nAmazon hiring tool: \nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/ \n\n\nMIT tech review(predictive policing): \nhttps://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol/#:~:text=It%27s%20no%20secret%20that%20predictive,lessen%20bias%20has%20little%20effect. \n\n\nClearview AI facial recognition:\nhttps://www.business-humanrights.org/es/%C3%BAltimas-noticias/clearview-ais-facial-recognition-technology-designed-for-surveillance-of-marginalized-groups-report-reveals/ \nhttps://www.forbes.com/sites/roberthart/2024/09/03/clearview-ai-controversial-facial-recognition-firm-fined-33-million-for-illegal-database/ \n\nGeneral privacy laws:\nhttps://www.dlapiperdataprotection.com/?t=law&c=US#:~:text=Under%20the%20comprehensive%20US%20state,of%20targeted%20advertising%20or%20profiling.\n\nMore examples: \nhttps://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples"
  },
  {
    "objectID": "Final6.html",
    "href": "Final6.html",
    "title": "PP Baseball Off Year or Regression to the Mean?",
    "section": "",
    "text": "The baseball team had a record year last year, reaching the finals in Cleveland, Ohio which is two rounds further than they had ever reached in program history. Yet this year the team spirit is that this season has been let down, beginning as a #5 seed in preseason ratings and have since dropped out of the top 35 nationally. This pattern repeats one seen only 3 years prior when the team had a phenomenal year and made a real push into the playoffs and then the following year was a disappointment as their win percentage dropped over 200 points and they secured 10 fewer wins. So the question then becomes are these “poor” seasons truly below average seasons? Or are these outstanding years the true outliers and the year following is purely regression towards the mean?\nThis was the question I set out to answer. Instead of scraping past season data I chose to utilize current stats that stated the win percentage of Pomona Pitzer vs their opponents. To do this I went into massey ratings (https://masseyratings.com/cbase2024/ncaa-d3/ratings) and found the expected winning percentage of PP baseball against every team they have played and intend to play this season. A note here is that I chose the win percentage that was calculated off of the 2024 season. I chose this data because I did not want the data to be flawed due to the games that had already occurred this season. I wanted to evaluate what Massey Ratings expected to occur for the 2025 season (when Pomona Pitzer Baseball was at its very best).\nUtilizing the win percentages I found on Massey Ratings and utilizing the Pomona Baseball schedule to extract who they played and how many times, I created the following table. A quick note, the win percentage is the percentage chance that Pomona Pitzer will beat them. Again these numbers have changed since Pomona has done poorly compared to expectations this season. For example, at the end of 2024 PP baseball had a 55% chance of beating La Verne, currently Massey Ratings says La Verne has a 57% chance of beating Pomona, (This is an extreme example as La Verne has had an exceptional season so far).\n\n\nCode\n# Create multi-line string with  data\nraw_text &lt;- \"\nCMS 0.56 3\nLa_Verne 0.55 4\nETBU 0.53 3\nOccidental 0.85 3\nCal_Lu 0.60 3\nCal_Tech 0.95 3\nWhittier 0.84 4\nLewis_and_Clark 0.71 3\nPacific_Lutheran 0.73 2\nChapman 0.60 3\nRedlands 0.74 3\nWilliamette 0.60 1\nMIT 0.80 2\nUW_La_Crosse 0.54 1\nTufts 0.65 3\n\"\n\n# Read the data into a data frame\nschedule &lt;- read.table(text = raw_text, header = FALSE, col.names = c(\"team\", \"win_prob\", \"games\"))\n\n\nprint(schedule)\n\n\n               team win_prob games\n1               CMS     0.56     3\n2          La_Verne     0.55     4\n3              ETBU     0.53     3\n4        Occidental     0.85     3\n5            Cal_Lu     0.60     3\n6          Cal_Tech     0.95     3\n7          Whittier     0.84     4\n8   Lewis_and_Clark     0.71     3\n9  Pacific_Lutheran     0.73     2\n10          Chapman     0.60     3\n11         Redlands     0.74     3\n12      Williamette     0.60     1\n13              MIT     0.80     2\n14     UW_La_Crosse     0.54     1\n15            Tufts     0.65     3\n\n\nWith the previous schedule and probability to data frame I then created the code to simulate the season. This is the original code that I would later turn into a function and map it in order to run it 1,000 times. (So these results are a sample simulated season.)\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Create a data frame with schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74,\n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulate the season\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    dplyr::rowwise() |&gt;\n    dplyr::mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    dplyr::ungroup()\n  \n  # Season totals\n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  list(results = results, total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation\nseason &lt;- simulate_season(schedule)\n\n# View detailed results\nprint(season$results)\n\n\n# A tibble: 14 × 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1\n\n\nCode\n# Print summary\ncat(\"\\nSimulated Season Summary:\\n\")\n\n\n\nSimulated Season Summary:\n\n\nCode\ncat(\"Total Wins:\", season$total_wins, \"\\n\")\n\n\nTotal Wins: 21 \n\n\nCode\ncat(\"Total Losses:\", season$total_losses, \"\\n\")\n\n\nTotal Losses: 19 \n\n\nTurning it into a function and running it 1000 times which is followed by some basic stats that we care about in this simulation: average wins, standard deviation, median and then it is interesting to see the min and max win seasons.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n#  original schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Your simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins=median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n# Print results\nprint(summary_stats)\n\n\n# A tibble: 1 × 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     27.7    2.78          28       36       19\n\n\nThe following graph is a presentation of all 1000 simulations and I made it shiny so that you can interact with it and understand percentage wise how a 27 win year such as this one is an expected year and not a poor showing. These calculations are also inflated as the win percentages were gathered off of the end of a historic PP season.\n\n\nCode\n# Prepare simulation data once\nset.seed(42)\n\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  tibble(total_wins = total_wins)\n}\n\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"Simulated Win Distribution\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"win_thresh\", \"Minimum Wins:\",\n                  min = min(sim_results$total_wins),\n                  max = max(sim_results$total_wins),\n                  value = 27,\n                  step = 1)\n    ),\n    mainPanel(\n      textOutput(\"percent_text\"),\n      plotOutput(\"win_plot\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  output$percent_text &lt;- renderText({\n    pct &lt;- mean(sim_results$total_wins &gt;= input$win_thresh) * 100\n    paste0(\"Percentage of seasons with a better record than \", input$win_thresh, \" wins: \", round(pct, 2), \"%\")\n  })\n  \n  output$win_plot &lt;- renderPlot({\n    ggplot(sim_results, aes(x = total_wins)) +\n      geom_bar(fill = \"steelblue\", color = \"black\") +\n      geom_vline(xintercept = input$win_thresh, color = \"red\", linetype = \"dashed\", size = 1) +\n      labs(\n        title = \"Distribution of Total Wins Over 1000 Simulated Seasons\",\n        x = \"Total Wins\",\n        y = \"Frequency\"\n      ) +\n      theme_minimal()\n  })\n}\n\n# Run app\nshinyApp(ui = ui, server = server)\n\n\nhttps://nicols18.shinyapps.io/simulation/ The above code relates to my shiny application.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Schedule data\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  tibble(total_wins = total_wins)\n}\n\n# Run 1000 simulations\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Create a bar plot of total wins\nggplot(sim_results, aes(x = total_wins)) +\n  geom_bar(fill = \"steelblue\", color = \"black\") +\n    geom_vline(xintercept = 27, color = \"red\", linetype = \"dashed\", size = 1)+ \n  labs(\n    title = \"Distribution of Total Wins Over 1000 Simulated Seasons\",\n    x = \"Total Wins\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\npct_27_or_more &lt;- mean(sim_results$total_wins &gt;= 27) * 100\nprint(paste0(\"Percentage of simulations with 27 or more wins: \", round(pct_27_or_more, 2), \"%\"))\n\n\n[1] \"Percentage of simulations with 27 or more wins: 65.7%\"\n\n\nThe red line represents the number of wins they had this season.\nAs can be seen in this graph the final result of 27 wins in the 2025 season is well aligned with the center of the bell curve. It could be considered a little low as the average expected number of wins was 27.7 and the median was 28 but still that is barely under performing. With this data I would suggest that this has not been a poor season but instead just slightly below what their expected number of wins should have been. Especially considering that these predictions were based off the greatest season in Pomona History so it could be safely assumed that these predictions are honestly too high. Last season the team had 30 regular season wins and yet after the season they were still predicted to only have won 28 games so that implies that they outperformed despite putting up exceptional offensive and defensive statistics.  So I decided to get some summary statistics utilizing the new predictive winning percentage after the 2025 season (seen below) and I found that even this year they outperformed what they were predicted to achieve. Albeit they overachieved by less than the 2024 team but it still shows how the 2025 team has had a solid year and to ever have considered it an “off year” is ridiculous. As I look to the future I wish them the best in the SCIAC playoffs as currently they are not guaranteed a regional spot with the new NPI rating system implemented by the NCAA.\n\n\nCode\nset.seed(42)  # for reproducibility\n\n# Updated schedule data\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.49, 0.43, 0.51, 0.84, 0.58, 0.93, 0.77,\n               0.64, 0.68, 0.72, 0.64, \n               0.75, 0.48, 0.71),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# Simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run the simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary statistics\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins = median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n# Print the summary\nprint(summary_stats)\n\n\n# A tibble: 1 × 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     26.3    2.79          26       34       17"
  },
  {
    "objectID": "SQL.html",
    "href": "SQL.html",
    "title": "Different Policing Across the Bay Bridge?",
    "section": "",
    "text": "As an Oakland native I have a very different understanding of the police than some of my peers. My understanding of the police in Oakland is that they are considerably overloaded so calling them for a crime that is not a current threat to someone’s life is often postponed severely. For example, I have actively seen someone underneath a neighbors car cutting out their catalytic converter and there was no point in calling the police as they would have no interest in such a crime. Even when our car was stolen they took hours to file a report and found it four days later after it had been trashed and discarded. For these reasons I was particularly interested in the percentage of stops that turned into searches and arrests as my understanding would be that a majority of the time they are making stop because they believe they will be taking some amount of action.\nSo my first action was to calculate the percentage of the time that stops turned into a search and the percentage of the time that a stop turned into arrest. This singular number for each was not especially interesting so I decided to see if there were any trends over time and created a scatterplot. This graph was created in intervals of four months because we only have data from 2013-2017. Towards the end we see a dramatic climb and I am very interested to know whether this climb continued. Note that the search rate gets up to nearly 40% in some periods and averages at 30% of the time.\n\n\nCode\nSELECT\n  CONCAT(YEAR(date), '-', LPAD(FLOOR((MONTH(date)-1)/4)*4 + 1, 2, '0')) AS period,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\nGROUP BY period\nORDER BY period\n\n\n\n\nCode\nggplot(oakland_rates, aes(x = period)) +\n  geom_point(aes(y = pct_search, color = \"Search Rate\"), size = 3) +\n  geom_line(aes(y = pct_search, color = \"Search Rate\"), group = 1) +\n  geom_point(aes(y = pct_arrest, color = \"Arrest Rate\"), size = 3) +\n  geom_line(aes(y = pct_arrest, color = \"Arrest Rate\"), group = 1) +\n  labs(\n    title = \"Search and Arrest Rates in Oakland Traffic Stops (2013–2017)\",\n    x = \"Period (4-month intervals)\",\n    y = \"Percentage (%)\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nI also wanted to include a secondary graph to illustrate that the number of stops overall is not correlated to these search and arrest rates which I found interesting. Yet with the normalization of the number of stops I felt it did not illustrate the percentages well which is why I have left both graphs.\n\n\nCode\noakland_rates &lt;- oakland_rates |&gt;\n  mutate(norm_total_stops = 100 * total_stops / max(total_stops))\n\nggplot(oakland_rates, aes(x = period)) +\n  geom_point(aes(y = pct_search, color = \"Search Rate\"), size = 3) +\n  geom_line(aes(y = pct_search, color = \"Search Rate\"), group = 1) +\n  geom_point(aes(y = pct_arrest, color = \"Arrest Rate\"), size = 3) +\n  geom_line(aes(y = pct_arrest, color = \"Arrest Rate\"), group = 1) +\n  geom_point(aes(y = norm_total_stops, color = \"Total Stops (Normalized)\"), size = 3) +\n  geom_line(aes(y = norm_total_stops, color = \"Total Stops (Normalized)\"), group = 1) +\n  labs(\n    title = \"Search, Arrest, and Total Stops in Oakland Traffic Stops (2013–2017)\",\n    x = \"Period (4-month intervals)\",\n    y = \"Percentage or Normalized Total\",\n    color = \"Metric\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nYet this information did not quite answer my question as a citation would also be warrant for action. So with some help I created a dataframe in which it counted whether any of the following had occurred on the traffic stop: citation, search, or arrest. As you can see in the table the percentage of stops with action was 65% which was honestly surprisingly low to me. There may be a different definition of what is considered a traffic stop and which stops are not included in this data but overall expected something higher.\n\n\nCode\nSELECT\n  COUNT(*) AS total_stops,\n  SUM(\n    CASE\n      WHEN arrest_made = TRUE OR search_conducted = TRUE OR citation_issued = TRUE THEN 1\n      ELSE 0\n    END\n  ) AS stops_with_action,\n  ROUND(100.0 * \n    SUM(\n      CASE\n        WHEN arrest_made = TRUE OR search_conducted = TRUE OR citation_issued = TRUE THEN 1\n        ELSE 0\n      END\n  ) / COUNT(*), 2) AS pct_with_action\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND arrest_made IS NOT NULL\n  AND search_conducted IS NOT NULL\n  AND citation_issued IS NOT NULL;\n\n\n\n\nCode\nprint(`citation/search`)\n\n\n  total_stops stops_with_action pct_with_action\n1      133405             87650            65.7\n\n\nThen I realized this data of search and arrest percentages are baseless without some comparisons. So I chose to compare Oakland to two other Bay Area cities. This would generally help keep constant laws, general size of the cities and overall sentiment of the police force. I compared Oakland to San Francisco (a 10 minute drive across the Bay Bridge) and San Jose (a 45 minute drive south).\n\n\nCode\nSELECT\n  'Oakland' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\nUNION ALL\n\nSELECT\n  'San Francisco' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_francisco_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\nUNION ALL\n\nSELECT\n  'San Jose' AS city,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_jose_2020_04_01\nWHERE search_conducted IS NOT NULL \n  AND arrest_made IS NOT NULL\n  AND date BETWEEN '2013-01-01' AND '2016-12-31'\n\n\n\n\nCode\nprint(city_rates)\n\n\n           city total_stops searches arrests pct_search pct_arrest\n1       Oakland      102690    30275   12353      29.48      12.03\n2 San Francisco      292469    13886    2734       4.75       0.93\n3      San Jose      111706    32998    9999      29.54       8.95\n\n\nThe sharp contrast between San Francisco and Oakland/San Jose was unbelievable to me. First thing to notice is San Francisco had nearly 3 times the number of stops as the others and this was after I accounted for the time periods and only included the overlapping years between all three. Yet despite having three times more total stops they had barely a third of the total searches and obviously a minimal search and arrest percentage. I was also surprised to see how similar San Jose’s numbers were to Oakland. I do not know San Jose well but my general understanding was that they had some “bad” areas but not enough to keep up with Oakland’s numbers. I am glad I made this comparison as I found it super interesting and it has left me with more questions to pursue about where these enormous differences stem from. Is it the system in each police force? Different laws? Less crime?\nLastly, it felt essential to take a look at race within the previous stats that I had already been looking at. Unfortunately, as it often shows, Black drivers and Hispanic drivers made up the largest proportion of searched drivers compared to white, asian and other in all three cities. So even in such diverse cities these biases and racism continues to persist in clear ways.\n\n\nCode\nSELECT\n  'Oakland' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_oakland_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\nUNION ALL\n\nSELECT\n  'San Francisco' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_francisco_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\n\nUNION ALL\n\nSELECT\n  'San Jose' AS city,\n  subject_race AS race,\n  COUNT(*) AS total_stops,\n  SUM(search_conducted = TRUE) AS searches,\n  SUM(arrest_made = TRUE) AS arrests,\n  ROUND(100.0 * SUM(search_conducted = TRUE) / COUNT(*), 2) AS pct_search,\n  ROUND(100.0 * SUM(arrest_made = TRUE) / COUNT(*), 2) AS pct_arrest\nFROM ca_san_jose_2020_04_01\nWHERE date BETWEEN '2013-03-01' AND '2017-12-31'\n  AND search_conducted IS NOT NULL\n  AND arrest_made IS NOT NULL\n  AND subject_race IS NOT NULL\nGROUP BY race\n\n\n\n\nCode\nggplot(city_race_rates, aes(x = city, y = pct_search, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Search Rate by Race (2013–2017)\",\n    x = \"City\",\n    y = \"Search Rate (%)\",\n    fill = \"Race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(city_race_rates, aes(x = city, y = pct_arrest, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Arrest Rate by Race (2013–2017)\",\n    x = \"City\",\n    y = \"Arrest Rate (%)\",\n    fill = \"Race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhat was more unbelievable was the stats on the percentage of stops by race for each city. This was truly eye opening on how biased the law enforcement system can be. I included the demographics of all three cities below the the following bar graph to truly illustrate how far from proportional these stops are. An insane stat I found is that currently there are 90,000 African Americans in Oakland in 2023 (in 2015 I believe there was less which makes this crazier), in the three years in which we are analyzing data the Oakland Police stopped nearly 78,000 African Americans. Of course there were repeats offenders but still the fact that it is possible that over 85% of African Americans in the city were stopped is absurd.\n\n\nCode\ncity_race_pct &lt;- city_race_rates |&gt;\n  group_by(city) |&gt;\n  mutate(pct_total_stops = 100 * total_stops / sum(total_stops))\n\n# Create the grouped bar chart\nggplot(city_race_pct, aes(x = city, y = pct_total_stops, fill = race)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Percentage of Stops by Race per City (2013–2017)\",\n    x = \"city\",\n    y = \"Percent of Total Stops\",\n    fill = \"race\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOakland Demographics: 30.51% White, 21.09% Black, 19.61% other race, 17.21% Hispanic, 15.54% Asian  San Francisco Demographics: 39.2% white, 34.4% Asian, 15.4% Hispanic, 8.4% other race, 5.2% Black  San Jose Demographics: 33.2% Hispanic, 32.8% Asian, 27.6% White, 2.8% Black, 3.6 % other\nThis demographics data was acquired from Data USA: https://datausa.io/profile/geo/oakland-ca/\nThis was truly an enlightening experience for me as I assumed there was still issues within Oakland’s system but I assumed they would be lesser because of the enormous diversity. Unfortunately that is not the case.\n\n\nCode\ndbDisconnect(con_traffic)\n\n\nData base from:\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "Presentation.html#section",
    "href": "Presentation.html#section",
    "title": "Changes to my Website",
    "section": "",
    "text": "Overall Changes I Made to my Website \nSubstantially improved my citations Added notes within my code to make it clearer *"
  },
  {
    "objectID": "Presentation.html#section-1",
    "href": "Presentation.html#section-1",
    "title": "Changes to my Website",
    "section": "",
    "text": "Creating a new Simulation Initially created a simulation about my odds to receive a single."
  },
  {
    "objectID": "Presentation.html#overall-changes-i-made-to-my-website",
    "href": "Presentation.html#overall-changes-i-made-to-my-website",
    "title": "Changes to my Website",
    "section": "Overall Changes I Made to my Website",
    "text": "Overall Changes I Made to my Website\n\nSubstantially improved my citations\n\nAdded notes within my code to make it clearer\nAdded code folding to all projects and overall cleaned up the presentation\nEdited introductions so they had a professional tone and appearance\nAdjusted home page to describe my current skills and career interests\nAdded Resume"
  },
  {
    "objectID": "Presentation.html#creating-a-new-simulation",
    "href": "Presentation.html#creating-a-new-simulation",
    "title": "Changes to my Website",
    "section": "Creating a new Simulation",
    "text": "Creating a new Simulation\n\nInitially created a simulation about my odds to receive a single.\n\nInsufficient information and an overload of variables hindered simulation value\nAdditionally overall lack of clarity throughout project\n\nInstead I chose to get a better understanding of the Pomona Baseball teams success\n\n\n\n\nPomona Pitzer Baseball sweeping ETBU to head to their first ever College World Series Appearance Photo/courtesy of Pomona-Pitzer athletics"
  },
  {
    "objectID": "Presentation.html#leading-question",
    "href": "Presentation.html#leading-question",
    "title": "Changes to my Website",
    "section": "Leading Question",
    "text": "Leading Question\n\nIs Pomona truly underperforming the year after a historic run?\n\n2022 historic run to regionals\n2023 poor season and missed SCIAC playoffs\n2024 greatest run in history to NCAA World Series in Cleveland\n2025 sentiments of a let down season (although ended up finishing strong)\n\n\n\n\n\nPomona Pitzer final game in Cleveland as they were eliminated by Endicott Photo/courtesy of Pomona-Pitzer athletics"
  },
  {
    "objectID": "Presentation.html#collecting-the-data",
    "href": "Presentation.html#collecting-the-data",
    "title": "Changes to my Website",
    "section": "Collecting the Data",
    "text": "Collecting the Data\n\nUtilized Massey Ratings to collect winning percentages https://masseyratings.com/cbase2024/ncaa-d3/ratings\n\n\n\n# A tibble: 14 × 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1"
  },
  {
    "objectID": "Presentation.html#collecting-the-data-and-simulating-the-season",
    "href": "Presentation.html#collecting-the-data-and-simulating-the-season",
    "title": "Changes to my Website",
    "section": "Collecting the Data and Simulating the Season",
    "text": "Collecting the Data and Simulating the Season\n\nUtilized Massey Ratings to collect winning percentages https://masseyratings.com/cbase2024/ncaa-d3/ratings\n\n\n\n# A tibble: 14 × 5\n   team             win_prob games  wins losses\n   &lt;chr&gt;               &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 CMS                  0.56     3     1      2\n 2 La Verne             0.55     4     1      3\n 3 ETBU                 0.53     3     1      2\n 4 Occidental           0.85     3     2      1\n 5 Cal Lu               0.6      3     2      1\n 6 Cal Tech             0.95     3     2      1\n 7 Whittier             0.84     4     2      2\n 8 Lewis and Clark      0.71     3     2      1\n 9 Pacific Lutheran     0.73     2     1      1\n10 Chapman              0.6      3     1      2\n11 Redlands             0.74     3     2      1\n12 MIT                  0.8      2     1      1\n13 UW La Crosse         0.54     1     1      0\n14 Tufts                0.65     3     2      1"
  },
  {
    "objectID": "Presentation.html#results-of-1000-simulations",
    "href": "Presentation.html#results-of-1000-simulations",
    "title": "Changes to my Website",
    "section": "Results of 1,000 Simulations",
    "text": "Results of 1,000 Simulations\n\nAfter mapping the simulation 1,000 times I found the following basic stats:\n\n\nset.seed(42)  # for reproducibility\n\n#  original schedule\nschedule &lt;- data.frame(\n  team = c(\"CMS\", \"La Verne\", \"ETBU\", \"Occidental\", \"Cal Lu\", \"Cal Tech\", \"Whittier\",\n           \"Lewis and Clark\", \"Pacific Lutheran\", \"Chapman\", \"Redlands\", \n           \"MIT\", \"UW La Crosse\", \"Tufts\"),\n  win_prob = c(0.56, 0.55, 0.53, 0.85, 0.60, 0.95, 0.84,\n               0.71, 0.73, 0.60, 0.74, \n               0.80, 0.54, 0.65),\n  games = c(3, 4, 3, 3, 3, 3, 4,\n            3, 2, 3, 3,\n            2, 1, 3)\n)\n\n# simulation function\nsimulate_season &lt;- function(schedule) {\n  results &lt;- schedule |&gt;\n    rowwise() |&gt;\n    mutate(\n      wins = sum(runif(games) &lt; win_prob),\n      losses = games - wins\n    ) |&gt;\n    ungroup()\n  \n  total_wins &lt;- sum(results$wins)\n  total_losses &lt;- sum(results$losses)\n  \n  tibble(total_wins = total_wins, total_losses = total_losses)\n}\n\n# Run simulation 1000 times\nn_simulations &lt;- 1000\nsim_results &lt;- map_dfr(1:n_simulations, ~simulate_season(schedule))\n\n# Summary\nsummary_stats &lt;- sim_results |&gt;\n  summarise(\n    avg_wins = mean(total_wins),\n    sd_wins = sd(total_wins),\n    median_wins=median(total_wins),\n    max_wins = max(total_wins),\n    min_wins = min(total_wins)\n  )\n\n\nprint(summary_stats)\n\n# A tibble: 1 × 5\n  avg_wins sd_wins median_wins max_wins min_wins\n     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;\n1     27.7    2.78          28       36       19"
  },
  {
    "objectID": "Presentation.html#graphical-presentation",
    "href": "Presentation.html#graphical-presentation",
    "title": "Changes to my Website",
    "section": "Graphical Presentation",
    "text": "Graphical Presentation\n\nCreated a shiny bar graph\n\nhttps://nicols18.shinyapps.io/simulation/"
  },
  {
    "objectID": "Presentation.html#conclusions",
    "href": "Presentation.html#conclusions",
    "title": "Changes to my Website",
    "section": "Conclusions",
    "text": "Conclusions\n\nFinal result of 27 wins aligns closely with the center of the simulated distribution.\nSlightly underperformed vs. expected average (27.7) and median (28), but the difference is minimal.\nPerformance is respectable, not disappointing—especially when considering predictions were based on the historic 2024 season.\nIn 2024, the team won 30 games, yet was predicted for 28—overperformed even with elite stats."
  },
  {
    "objectID": "Presentation.html#how-massey-ratings-looks",
    "href": "Presentation.html#how-massey-ratings-looks",
    "title": "Changes to my Website",
    "section": "How Massey Ratings Looks",
    "text": "How Massey Ratings Looks\nhttps://masseyratings.com/game.php?s0=614639&oid0=6286&h=0&s1=614639&oid1=0 &lt;/p&gt;"
  },
  {
    "objectID": "Presentation.html#will-pomona-get-a-regional-bid",
    "href": "Presentation.html#will-pomona-get-a-regional-bid",
    "title": "Changes to my Website",
    "section": "Will Pomona get a Regional Bid?",
    "text": "Will Pomona get a Regional Bid?\n\nGuaranteed bid if they win SCIAC Playoffs starting today at 3pm\nTop 23 teams in terms of NPI that do not win their conference get an at large bid\n64 total teams make the playoffs\nNPI has a final calculation on May 11th\nPomona is currently 54th nationally in NPI. Will they get a bid?"
  },
  {
    "objectID": "Presentation.html#massey-ratings",
    "href": "Presentation.html#massey-ratings",
    "title": "Changes to my Website",
    "section": "Massey Ratings ",
    "text": "Massey Ratings \nhttps://masseyratings.com/game.php?s0=614639&oid0=6286&h=0&s1=614639&oid1=0 &lt;/p&gt;"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "cat('&lt;iframe src=\"Resumewebsite.pdf\" width=\"100%\" height=\"800px\" style=\"border:none;\"&gt;&lt;/iframe&gt;')"
  }
]