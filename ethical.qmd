---
title: "Ethics within Data Science"
author: "Nicolas Laub-Sabater"
---

04/16/2025

Title: Ethical Reflections on the Amazon AI Hiring Tool

Amazon’s now-discontinued AI hiring tool offers a powerful example of how data science can unintentionally perpetuate systemic bias when not designed and tested with ethical rigor. Developed to streamline hiring, the system was trained on ten years’ worth of resumes submitted primarily by men—an input that reflected the company’s historically male-dominated workforce. As a result, the model penalized resumes that contained terms like “women’s” or references to all-women’s colleges, thereby disadvantaging female applicants. While the tool was built with the intent to improve hiring efficiency, the flawed composition of its training data allowed past inequalities to become codified in its algorithmic logic.

This case exemplifies the ethical mandate to “Recognize and mitigate bias in ourselves and in the data we use.” By training the model on biased historical data, Amazon effectively taught the system to mimic prior discriminatory hiring patterns. The algorithm's design assumed that prior successful candidates were ideal representations of merit, when in reality, that dataset excluded many qualified individuals due to historical gender bias. Amazon's failure to interrogate the representativeness of its training data before deploying the system shows how technical sophistication cannot substitute for ethical foresight. Bias in the data is not a peripheral concern—it is central to whether machine learning systems serve or harm society.

Another relevant ethical principle is to “Consider carefully the ethical implications of choices we make when using data, and the impacts of our work on individuals and society.” The tool’s outputs directly influenced the opportunities afforded to real applicants, reinforcing structural inequalities in hiring. The decision to automate a process historically shaped by bias—without first addressing that bias—resulted in a system that actively marginalized qualified candidates. Although the tool promised increased objectivity, it ultimately reproduced the very gender disparities that hiring automation should help eliminate. This underscores the importance of interrogating how data collection and model design choices translate into real-world consequences.

A further lesson from this example relates to the principle: “Be open to changing our methods and conclusions in response to new knowledge.” After internal teams discovered the model’s gender bias, Amazon chose not to deploy the system further. This decision, though belated, demonstrates an openness to reevaluating a flawed product in light of ethical concerns. In contrast to organizations that double down on problematic technologies, Amazon’s willingness to discontinue the project illustrates the need for ongoing critical review and the flexibility to pivot when systems fail to meet fairness standards. The ethical use of data science requires continual reassessment—not just during development, but also after deployment, as societal impacts become visible.

Finally, the Amazon case also invites reflection on the question: “Who was measured? Are those individuals representative of the people to whom we’d like to generalize or apply the algorithm?” The training data consisted largely of resumes from male candidates who had previously been hired by Amazon. These individuals formed the benchmark against which new applicants were evaluated, reinforcing an unrepresentative hiring standard. Women and other underrepresented groups were effectively sidelined by a model designed to replicate historical success, rather than broaden it. This mismatch between the training population and the intended applicant pool highlights the dangers of generalizing from narrow or biased datasets. Without deliberate efforts to ensure that training data reflects the diversity of the modern workforce, algorithmic tools risk becoming vehicles for institutionalized exclusion.

In sum, the Amazon AI hiring tool illustrates the necessity of embedding ethical reflection into every stage of the data science lifecycle—from data selection to deployment. Recognizing bias, evaluating societal impact, adapting to new information, and scrutinizing the representativeness of our datasets are not just technical tasks; they are moral imperatives. If we are to build AI systems that genuinely advance equity and opportunity, we must approach data science as both a technical and ethical endeavor.

\
Furthering the above content, my mom came across another article that I found particularly compelling and relevant to these ethical concerns. As a result, I’ve decided to include an additional discussion addressing some of the earlier questions and data ethics values, this time focusing on a new example: the facial recognition AI developed by Clearview AI. Clearview built its database by scraping billions of images from social media platforms—without the consent of the platforms or the individuals depicted. This unauthorized data collection has led to Clearview being fined tens of millions of dollars by multiple countries for violating privacy laws. Clearly, this data should not be used, as it was obtained without informed consent. Despite this, the U.S. government has not only refrained from penalizing Clearview but has actively adopted its technology in major federal agencies, including Immigration and Customs Enforcement (ICE) and the FBI.

Clearview’s technology is primarily deployed in law enforcement contexts and disproportionately targets marginalized populations, particularly along the U.S. border. One major ethical concern is the reinforcement of a surveillance feedback loop: over-policing certain communities leads to more recorded violations, which in turn justifies increased surveillance. This cycle perpetuates negative stereotypes—particularly of migrants—and may even serve institutional goals like securing greater funding for agencies like ICE. Perhaps most troubling is that the data powering Clearview’s technology was collected entirely without consent. The company has refused to disclose where the images came from, although it is widely understood that they were scraped from social media platforms.

Despite global condemnation and legal action from other countries, the U.S. government’s continued use of Clearview’s software raises serious concerns about the future of data privacy. While some states have introduced privacy laws, the absence of comprehensive federal legislation is striking. This gap is partly due to political gridlock, but also reflects what legal scholars have called the “increasing complexity of privacy concerns” (DLA Piper), which has made it difficult to craft and pass unified legislation. As a result, privacy protections in the U.S. remain fragmented and insufficient, with meaningful regulation often stalled or dismissed. Unless these issues are prioritized, we may not see robust federal privacy protections enacted anytime soon—an alarming prospect given the scale and scope of modern data collection practices.

\
\

-   Dastin, Jeffrey. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.” Reuters, 10 Oct. 2018, [www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G](http://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G).

-   Hao, Karen. “Predictive Policing Algorithms Are Racist. They Need to Be Dismantled.” MIT Technology Review, 5 Feb. 2021, [www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol](http://www.technologyreview.com/2021/02/05/1017560/predictive-policing-racist-algorithmic-bias-data-crime-predpol).

-   “Clearview AI’s Facial Recognition Technology Designed for Surveillance of Marginalized Groups, Report Reveals.” Business & Human Rights Resource Centre, www.business-humanrights.org/en/latest-news/clearview-ais-facial-recognition-technology-designed-for-surveillance-of-marginalized-groups-report-reveals.

-   Hart, Robert. “Clearview AI: Controversial Facial Recognition Firm Fined \$33 Million for Illegal Database.” Forbes, 3 Sept. 2024, [www.forbes.com/sites/roberthart/2024/09/03/clearview-ai-controversial-facial-recognition-firm-fined-33-million-for-illegal-database](http://www.forbes.com/sites/roberthart/2024/09/03/clearview-ai-controversial-facial-recognition-firm-fined-33-million-for-illegal-database).

-   DLA Piper. “United States - Data Protection Overview.” DLA Piper Data Protection Laws of the World, [www.dlapiperdataprotection.com/?t=law&c=US](http://www.dlapiperdataprotection.com/?t=law&c=US).

-   IBM. “Shedding Light on AI Bias with Real World Examples.” IBM Think Blog, [www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples](http://www.ibm.com/think/topics/shedding-light-on-ai-bias-with-real-world-examples).
